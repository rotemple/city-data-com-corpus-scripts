{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c29fac-8168-4f4a-9e37-c74efccc6995",
   "metadata": {},
   "source": [
    "## Text Processing Utilities for City-Data Corpus\n",
    "\n",
    "The functions below were used to normalize the City-Data Corpus texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f8ac6b-9ef3-450f-baaa-10306caa5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans, SpectralClustering, kmeans_plusplus\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import pickle\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "from plotly.offline import plot\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_short,strip_non_alphanum, strip_tags,strip_multiple_whitespaces, preprocess_documents, preprocess_string, strip_numeric, remove_stopwords, strip_tags, strip_punctuation, stem_text\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def flatten_list(somelist):\n",
    "    \"\"\"\"\n",
    "    Function to flatten a list of lists.\n",
    "\n",
    "    Args:\n",
    "        somelist: List of lists.\n",
    "\n",
    "    Returns:\n",
    "        Merged list\n",
    "    \"\"\"\n",
    "    if any(isinstance(el, list) for el in somelist) == False:\n",
    "        return somelist\n",
    "    flat_list = list(itertools.chain(*somelist))\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "stops = stopwords.words('english') + ['said','know','maybe','post','advertisements','advertisement','posted','thread','like','could','should','would','thing']\n",
    "wn = WordNetLemmatizer()\n",
    "stemmer = nltk.PorterStemmer()\n",
    "def text_process(text):\n",
    "    \"\"\"\n",
    "    Function to normalize text.\n",
    "\n",
    "    Args:\n",
    "        texts: string\n",
    "\n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if not re.findall(r'\\.com|___|Advertisements|-|_',token)]\n",
    "\n",
    "    return ' '.join([wn.lemmatize(token) for token in tokens if len(token) > 3 and token not in stops and not re.findall(r'[0-9]',token) and not re.findall(r'htt',token)])\n",
    "\n",
    "def make_graph_citydata(dataframe):\n",
    "    \"\"\"\n",
    "    Generates networkx graph of City-Data.com Corpus Forum Data\n",
    "\n",
    "    Args:\n",
    "        dataframe: City-Data.com Corpus Dataframe\n",
    "\n",
    "    Returns:\n",
    "        networkx Graph\n",
    "    \"\"\"\n",
    "    edges = [(str(x),str(y)) for (x,y) in list(zip(dataframe.post_id.tolist(),dataframe.quote_id.tolist()))]\n",
    "    G = nx.MultiDiGraph()\n",
    "    for i in range(len(dataframe)):\n",
    "        G.add_node(dataframe.iloc[i]['post_id'],text=dataframe.iloc[i]['post'])\n",
    "                 \n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe.iloc[i]['quote_id'] != '' and dataframe.iloc[i]['quote_id'] not in G.nodes():\n",
    "            try:\n",
    "                G.add_node(dataframe.iloc[i]['quote_id'],text=dataframe.iloc[i]['quote'])\n",
    "            except:\n",
    "                G.add_node(dataframe.iloc[i]['quote_id'],text=None)\n",
    "        \n",
    "    G.add_edges_from(edges)\n",
    "    try:\n",
    "        G.remove_node('')\n",
    "    except:\n",
    "        pass\n",
    "    return G\n",
    "\n",
    "def get_paths_city_data(dataframe):\n",
    "    \"\"\"\n",
    "    Function to extract threaded posts from a Pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: City-Data.com Corpus DataFrame\n",
    " \n",
    "    Returns:\n",
    "        networkx graph, list of threads\n",
    "    \"\"\"\n",
    "    G = make_graph_citydata(dataframe)\n",
    "    sink_nodes = [node for node, outdegree in dict(G.out_degree(G.nodes())).items() if outdegree == 0]\n",
    "    source_nodes = [node for node, indegree in dict(G.in_degree(G.nodes())).items() if indegree == 0]\n",
    "    ss_nodes = [(source, sink) for sink in sink_nodes for source in source_nodes]\n",
    "    paths = []\n",
    "    for (source,sink) in ss_nodes:\n",
    "        for path in nx.all_simple_paths(G, source=source, target=sink):\n",
    "            paths.append(path)\n",
    "    return G, paths\n",
    "\n",
    "def make_thread_embeddings(dataframe, model):\n",
    "    \"\"\"\n",
    "    Function to convert City-Data.com Corpus posts and quoted posts into a network graph and embeddings.\n",
    "\n",
    "    Args:\n",
    "        dataframe: City-Data.com Corpus Dataframe\n",
    "        model: sentence-transformer model\n",
    "\n",
    "    Returns:\n",
    "        networkx graph\n",
    "        City-Data.com Corpus threads and singleton posts\n",
    "        City-Data.com Corpus thread and post embeddings\n",
    "    \"\"\"\n",
    "    dataframe.fillna('',inplace=True)\n",
    "    id_text = {}\n",
    "    for i in range(len(dataframe)):\n",
    "        \n",
    "        id_text[dataframe.iloc[i]['quote_id']] = dataframe.iloc[i]['quote']\n",
    "        id_text[dataframe.iloc[i]['post_id']] = dataframe.iloc[i]['post']\n",
    "\n",
    "    G, paths = get_paths_city_data(dataframe)\n",
    "    chains = []\n",
    "    for path in paths:\n",
    "        p = []\n",
    "        for x in path:\n",
    "            try:\n",
    "                p.append(id_text[x])\n",
    "            except:\n",
    "                p.append('')\n",
    "        chains.append(p)\n",
    "    joint_chains = [' '.join(chain) for chain in chains]\n",
    "    embeddings = model.encode(joint_chains)\n",
    "    singletons = [node for node in G.nodes() if node not in flatten_list(paths)]\n",
    "    singleton_embeddings = model.encode([id_text[s] for s in singletons])\n",
    "    singleton_texts = [id_text[s] for s in singletons]\n",
    "    return G, paths, joint_chains, embeddings, singletons, singleton_embeddings,singleton_texts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791f06d-c661-4798-8a0c-0e62f8b839d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, paths = get_paths_city_data(citydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f2a30-5e09-4d49-a697-74b302607dcb",
   "metadata": {},
   "source": [
    "## Gensim LDA Modeling Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b8b70d-2c04-4fbb-819d-e8daeda2011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "def gensim_lda(texts, topic_num=5, topic_word_priors=None,numwords=25, eta_=None, tfidf=False):\n",
    "    \"\"\"\n",
    "    Gensim lda wrapper with guided topic modeling.\n",
    "\n",
    "    Args:\n",
    "        texts: list of strings\n",
    "        topic_num: (int) number of topics\n",
    "        topic_word_priors: list of words (string) to guide modeling\n",
    "        numwords: (int) number of topical terms\n",
    "        eta_: None or list of ints\n",
    "        tfidf: (bool) if True, then use gensim tfidf term weighting (default is False)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if tfidf != False:\n",
    "        #process texts\n",
    "        #build gensim dictionary\n",
    "        processed_texts = [text_tokenize(text) for text in texts]\n",
    "        dictionary = gensim.corpora.Dictionary(processed_texts)\n",
    "    \n",
    "        #build bag-of-words representation\n",
    "        bow = [dictionary.doc2bow(text.split()) for text in texts]\n",
    "        tfidf = models.TfidfModel(bow)\n",
    "        corpus_tfidf = tfidf[bow]\n",
    "        #guided lda with eta\n",
    "        if topic_word_priors and eta_ != None:\n",
    "            etas = []\n",
    "        \n",
    "            for r in range(len(topic_word_priors)):\n",
    "                eta = []\n",
    "                for i in range(len(dictionary)):\n",
    "                    \n",
    "                    if dictionary[i] in topic_word_priors[r]:\n",
    "                        eta.append(np.array(eta_))\n",
    "                    else:\n",
    "                        eta.append(np.array(1/topic_num))\n",
    "                etas.append(eta)\n",
    "    \n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=corpus_tfidf, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=np.array(etas),\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "    \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[corpus_tfidf]\n",
    "    \n",
    "            #extract topical terms\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=numwords)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "            \n",
    "        else:\n",
    "            #standard lda\n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=corpus_tfidf, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=None,\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "            \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[corpus_tfidf]\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=numwords)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "    else:\n",
    "        #process texts\n",
    "        #build gensim dictionary\n",
    "        processed_texts = [text_tokenize(text) for text in texts]\n",
    "        dictionary = gensim.corpora.Dictionary(processed_texts)\n",
    "        \n",
    "        #build bag-of-words representation\n",
    "        bow = [dictionary.doc2bow(text.split()) for text in texts]\n",
    "        #tfidf = models.TfidfModel(bow)\n",
    "        #corpus_tfidf = tfidf[bow]\n",
    "    \n",
    "    \n",
    "        #guided lda with eta\n",
    "        if topic_word_priors and eta_ != None:\n",
    "            etas = []\n",
    "        \n",
    "            for r in range(len(topic_word_priors)):\n",
    "                eta = []\n",
    "                for i in range(len(dictionary)):\n",
    "                    \n",
    "                    if dictionary[i] in topic_word_priors[r]:\n",
    "                        eta.append(np.array(eta_))\n",
    "                    else:\n",
    "                        eta.append(np.array(1/topic_num))\n",
    "                etas.append(eta)\n",
    "    \n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=bow, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=np.array(etas),\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "    \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[bow]\n",
    "    \n",
    "            #extract topical terms\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=numwords)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "        \n",
    "        else:\n",
    "        #standard lda\n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=bow, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=None,\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "            \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[bow]\n",
    "            \n",
    "            #extract topical terms\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=25)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdc0b3-1719-4031-88d9-28dc8d9349e2",
   "metadata": {},
   "source": [
    "## City-Data.com Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8688cc-b97b-47e2-87e6-a3f4770dfd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load City-Data.com Corpus from Zenodo\n",
    "citydata = pd.read_csv(\"https://zenodo.org/records/10086354/files/citydata.csv?download=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f317428-2aa8-4f0a-ac7e-455cc9285198",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentence Embedding-Based Topic Modeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0a2e8a1-43af-4cae-9b3c-4966b1ef2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans, SpectralClustering, kmeans_plusplus\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def topic_model(texts, transformer, clusters=3,vectorizer='cv',mindf=5, ngrams=(1,1),kbest=5000,init_='k-means++'):\n",
    "    \"\"\"\n",
    "    Function to SE-Topic Model City-Data.com Corpus.\n",
    "\n",
    "    Args:\n",
    "        texts: list of strings\n",
    "        transformer: sentence-transformer model \n",
    "        clusters: (int) number of topics to derive\n",
    "        vectorizer: (string) 'cv' or 'tfidf'\n",
    "        mindf: (int) minimum threshold for token inclusion\n",
    "        ngrams: (tuple) ngram range for tokens\n",
    "        kbest: max textual features for topic modeling\n",
    "        init_: string or list of topic priors\n",
    "    Returns:\n",
    "        dictionary: dataframe of texts, topical term weights, vectroizer, clusterer, topics\n",
    "    \"\"\"\n",
    "        \n",
    "    embeddings = transformer.encode(texts)\n",
    "    #if cluster centers provided:\n",
    "    if type(init_) == np.ndarray:\n",
    "        print('setting cluster centers')\n",
    "    \n",
    "    #instantiate kmeans clusterer \n",
    "    km = KMeans(n_clusters=clusters,init=init_, random_state=0)\n",
    "    km.fit(embeddings)\n",
    "\n",
    "    #process texts for topical word extraction\n",
    "    processed_threads = [text_process(text) for text in texts]\n",
    "\n",
    "    #group text by kmeans cluster label\n",
    "    df = pd.DataFrame({'text':processed_threads,'label':km.labels_})    \n",
    "    df_grouped = df.groupby('label')['thread'].apply(list)\n",
    "\n",
    "    if vectorizer == 'cv': #sklearn CountVectorizer\n",
    "        cv = CountVectorizer(stop_words='english',min_df=mindf,ngram_range=ngrams)\n",
    "        CX = cv.fit_transform(df.text)\n",
    "        \n",
    "        #select kbest features\n",
    "        kbc = SelectKBest(chi2, k=kbest).fit(CX, km.labels_)\n",
    "\n",
    "        ff = pd.DataFrame()\n",
    "        ff['term'] = np.asarray(cv.get_feature_names_out())[kbc.get_support()]\n",
    "        #extract topical terms\n",
    "        if clusters > 2:\n",
    "            for i in range(clusters):\n",
    "                ff['coef_'+str(i)] = kbc.transform(cv.transform([' '.join(df.groupby('label')['text'].apply(list)[i])])).toarray().tolist()[0]\n",
    "                                                 \n",
    "        else:\n",
    "            ff['coef_0'] = kbc.transform(cv.transform([' '.join(df.groupby('label')['text'].apply(list)[i])])).toarray().tolist()[0]\n",
    "            ff['coef_1'] = kbc.transform(cv.transform([' '.join(df.groupby('label')['text'].apply(list)[i])])).toarray().tolist()[0]\n",
    "            \n",
    "        ff = pd.DataFrame([ff.iloc[i] for i in range(len(ff)) if len(ff.iloc[i]['term']) > 3])\n",
    "        topics = get_topical_terms(ff,term_count=25)\n",
    "        print(get_topical_coherence(processed_threads, topics,metric='u_mass'))\n",
    "        return {'texts':df,'term_weights':ff,'vectorizer':cv, 'clusterer':km, 'topics':topics}\n",
    "   \n",
    "    elif vectorizer == 'tfidf':\n",
    "        tfidf = TfidfVectorizer(stop_words='english',min_df=mindf, ngram_range=ngrams)\n",
    "        CX = tfidf.fit_transform(df.thread)\n",
    "        kbc = SelectKBest(chi2, k=kbest).fit(CX, km.labels_)\n",
    "\n",
    "        ff = pd.DataFrame()\n",
    "        ff['term'] = np.asarray(tfidf.get_feature_names_out())[kbc.get_support()]\n",
    "        if clusters > 2:\n",
    "            for i in range(clusters):\n",
    "                ff['coef_'+str(i)] = kbc.transform(tfidf.transform([' '.join(df.groupby('label')['text'].apply(list)[i])])).toarray().tolist()[0]\n",
    "                                                 \n",
    "        else:\n",
    "            ff['coef_0'] = kbc.transform(tfidf.transform([' '.join(df.groupby('label')['text'].apply(list)[i])])).toarray().tolist()[0]\n",
    "            ff['coef_1'] = kbc.transform(tfidf.transform([' '.join(df.groupby('label')['text'].apply(list)[i])])).toarray().tolist()[0]\n",
    "            \n",
    "        ff = pd.DataFrame([ff.iloc[i] for i in range(len(ff)) if len(ff.iloc[i]['term']) > 3])\n",
    "        topics = get_topical_terms(ff,term_count=25)\n",
    "        print(get_topical_coherence(processed_threads, topics,metric='u_mass'))\n",
    "\n",
    "        return {'texts':df,'term_weights':ff,'vectorizer':tfidf, 'clusterer':km, 'topics':topics}\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#instantiate transformer model to create embeddings\n",
    "smodel = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed305d-965e-44eb-b315-09e32935c168",
   "metadata": {},
   "source": [
    "### Default Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefec779-31fb-4eb4-904b-b18516922b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY, city_paths, city_chains, city_embeddings, city_singletons,city_singleton_embeddings, city_singleton_texts = make_thread_embeddings(citydata,smodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3581fed-5a26-4124-ad1c-1980e2c875e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = topic_model(texts, smodel, clusters=4,vectorizer='cv',mindf=5, ngrams=(1,1),kbest=5000,init_='k-means++'):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0c02c-1bda-4907-bb71-655409fbec5f",
   "metadata": {},
   "source": [
    "## Gensim Topic Modeling Functions\n",
    "\n",
    "The following functions were used to derive LDA and guided LDA topic models in gensim for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f67b4-8e29-448e-86c4-bcd8c630e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "#custom text normalizer for City-Data Corpus\n",
    "def text_tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    tokens = [token for token in tokens if not re.findall(r'\\.com|___|Advertisements|-|_',token)]\n",
    "    lemmas = [wn.lemmatize(token) for token in tokens if len(token) > 2 and token not in stops]\n",
    "    filtered = [lemma for lemma in lemmas if not re.findall(r'[0-9]',lemma) and not re.findall(r'htt',lemma)]\n",
    "    return filtered\n",
    "    \n",
    "def gensim_lda(texts, topic_num=5, topic_word_priors=None,numwords=25, eta_=None, tfidf=False):\n",
    "    \"\"\"gensim lda wrapper with guided topic modeling\"\"\"\n",
    "\n",
    "    if tfidf != False:\n",
    "        #process texts\n",
    "        #build gensim dictionary\n",
    "        processed_texts = [text_tokenize(text) for text in texts]\n",
    "        dictionary = gensim.corpora.Dictionary(processed_texts)\n",
    "    \n",
    "        #build bag-of-words representation\n",
    "        bow = [dictionary.doc2bow(text.split()) for text in texts]\n",
    "        tfidf = models.TfidfModel(bow)\n",
    "        corpus_tfidf = tfidf[bow]\n",
    "        #guided lda with eta\n",
    "        if topic_word_priors and eta_ != None:\n",
    "            etas = []\n",
    "        \n",
    "            for r in range(len(topic_word_priors)):\n",
    "                eta = []\n",
    "                for i in range(len(dictionary)):\n",
    "                    \n",
    "                    if dictionary[i] in topic_word_priors[r]:\n",
    "                        eta.append(np.array(eta_))\n",
    "                    else:\n",
    "                        eta.append(np.array(1/topic_num))\n",
    "                etas.append(eta)\n",
    "    \n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=corpus_tfidf, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=np.array(etas),\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "    \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[corpus_tfidf]\n",
    "    \n",
    "            #extract topical terms\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=numwords)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "            \n",
    "        else:\n",
    "            #standard lda\n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=corpus_tfidf, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=None,\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "            \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[corpus_tfidf]\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=numwords)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "    else:\n",
    "        #process texts\n",
    "        #build gensim dictionary\n",
    "        processed_texts = [text_tokenize(text) for text in texts]\n",
    "        dictionary = gensim.corpora.Dictionary(processed_texts)\n",
    "        \n",
    "        #build bag-of-words representation\n",
    "        bow = [dictionary.doc2bow(text.split()) for text in texts]\n",
    "        #tfidf = models.TfidfModel(bow)\n",
    "        #corpus_tfidf = tfidf[bow]\n",
    "    \n",
    "    \n",
    "        #guided lda with eta\n",
    "        if topic_word_priors and eta_ != None:\n",
    "            etas = []\n",
    "        \n",
    "            for r in range(len(topic_word_priors)):\n",
    "                eta = []\n",
    "                for i in range(len(dictionary)):\n",
    "                    \n",
    "                    if dictionary[i] in topic_word_priors[r]:\n",
    "                        eta.append(np.array(eta_))\n",
    "                    else:\n",
    "                        eta.append(np.array(1/topic_num))\n",
    "                etas.append(eta)\n",
    "    \n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=bow, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=np.array(etas),\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "    \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[bow]\n",
    "    \n",
    "            #extract topical terms\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=numwords)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "        \n",
    "        else:\n",
    "        #standard lda\n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=bow, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=None,\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "            \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[bow]\n",
    "            \n",
    "            #extract topical terms\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=25)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892f40d-e613-4def-9281-51c090548358",
   "metadata": {},
   "source": [
    "## Network Graph Functions\n",
    "\n",
    "The following functions were used to convert City-Data.com Corpus forum data into network graphs of posts and replies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d35c15-f9b2-42b4-9b1f-dcd0cafc3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def make_graph_citydata(dataframe):\n",
    "    edges = [(str(x),str(y)) for (x,y) in list(zip(dataframe.post_id.tolist(),dataframe.quote_id.tolist()))]\n",
    "    G = nx.MultiDiGraph()\n",
    "    for i in range(len(dataframe)):\n",
    "        G.add_node(dataframe.iloc[i]['post_id'],text=dataframe.iloc[i]['post'])\n",
    "                 \n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe.iloc[i]['quote_id'] != '' and dataframe.iloc[i]['quote_id'] not in G.nodes():\n",
    "            try:\n",
    "                G.add_node(dataframe.iloc[i]['quote_id'],text=dataframe.iloc[i]['quote'])\n",
    "            except:\n",
    "                G.add_node(dataframe.iloc[i]['quote_id'],text=None)\n",
    "        \n",
    "    G.add_edges_from(edges)\n",
    "    G.remove_node('')\n",
    "    \n",
    "    return G\n",
    "\n",
    "def get_paths_city_data(dataframe):\n",
    "    G = make_graph_citydata(dataframe)\n",
    "    sink_nodes = [node for node, outdegree in dict(G.out_degree(G.nodes())).items() if outdegree == 0]\n",
    "    source_nodes = [node for node, indegree in dict(G.in_degree(G.nodes())).items() if indegree == 0]\n",
    "    ss_nodes = [(source, sink) for sink in sink_nodes for source in source_nodes]\n",
    "    paths = []\n",
    "    for (source,sink) in ss_nodes:\n",
    "        for path in nx.all_simple_paths(G, source=source, target=sink):\n",
    "            paths.append(path)\n",
    "    return G, paths\n",
    "\n",
    "def make_thread_embeddings(dataframe, model):\n",
    "    id_text = {}\n",
    "    for i in range(len(dataframe)):\n",
    "        \n",
    "        id_text[dataframe.iloc[i]['quote_id']] = dataframe.iloc[i]['quote']\n",
    "        id_text[dataframe.iloc[i]['post_id']] = dataframe.iloc[i]['post']\n",
    "\n",
    "    G, paths = get_paths_city_data(dataframe)\n",
    "    chains = []\n",
    "    for path in paths:\n",
    "        p = []\n",
    "        for x in path:\n",
    "            try:\n",
    "                p.append(id_text[x])\n",
    "            except:\n",
    "                p.append('')\n",
    "        chains.append(p)\n",
    "    joint_chains = [' '.join(chain) for chain in chains]\n",
    "    embeddings = model.encode(joint_chains)\n",
    "    singletons = [node for node in G.nodes() if node not in flatten_list(paths)]\n",
    "    singleton_embeddings = model.encode([id_text[s] for s in singletons])\n",
    "    singleton_texts = [id_text[s] for s in singletons]\n",
    "    return G, paths, joint_chains, embeddings, singletons, singleton_embeddings,singleton_texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
