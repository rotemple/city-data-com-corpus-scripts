{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JuumjOZ_AiAw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from __future__ import division\n",
    "import pickle\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "#from sentence_transformers import SentenceTransformer, util\n",
    "#import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "from plotly.offline import plot\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_short,strip_non_alphanum, strip_tags,strip_multiple_whitespaces, preprocess_documents, preprocess_string, strip_numeric, remove_stopwords, strip_tags, strip_punctuation, stem_text\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def flatten_list(somelist):\n",
    "        if any(isinstance(el, list) for el in somelist) == False:\n",
    "            return somelist\n",
    "        flat_list = list(itertools.chain(*somelist))\n",
    "        return flat_list\n",
    "\n",
    "\n",
    "def open_pickle(fname):\n",
    "    with open(fname,'rb') as s:\n",
    "        data = pickle.load(s)\n",
    "    return data\n",
    "\n",
    "def save_pickle(f, filename):\n",
    "    with open(filename, 'wb') as h:\n",
    "        pickle.dump(f,h,2)\n",
    "\n",
    "def remove_urls (text):\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|-)*\\b|http.*?(?=\\s)', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'(www\\.)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|-)*\\b|http.*?(?=\\s)', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    return(text)\n",
    "\n",
    "def remove_mentions (vTEXT):\n",
    "    vTEXT = re.sub(r'(@\\w+)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)\n",
    "\n",
    "def remove_smartquotes(s):\n",
    "    return re.subn(r'”|“|‘|’','',s)[0]\n",
    "\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), remove_urls, remove_mentions, strip_tags,\n",
    "                  strip_punctuation, remove_stopwords, strip_numeric, strip_short,\n",
    "                  remove_smartquotes,strip_multiple_whitespaces,stem_text, strip_non_alphanum]\n",
    "\n",
    "def strip_quoted_post(post):\n",
    "  if re.search('Quote:',post):\n",
    "    try:\n",
    "      s = post.split('Posted by')[1]\n",
    "      f = s.split('</a>')[1]\n",
    "    except IndexError:\n",
    "\n",
    "      f = post.split('</a>')[0]\n",
    "\n",
    "    return f\n",
    "  else:\n",
    "    return post\n",
    "\n",
    "def normalize_text(text):\n",
    "    page = soup(text)\n",
    "    anchors = page.findAll('a')\n",
    "    for a in anchors:\n",
    "        a.decompose()\n",
    "\n",
    "    strongs = page.findAll('strong')\n",
    "    for s in strongs:\n",
    "        s.decompose()\n",
    "\n",
    "    t = remove_urls(page.text)\n",
    "    ft = re.subn(r'Quote:|Originally Posted by|<a href=|>|</a|\\n|\\t|\\r','',t)[0].strip()\n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Siez18e1pWjx"
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "def draw_networkx_plotly(GRAPH,title='Network Graph',layout='spring_layout',scale=1,):\n",
    "  \"\"\"\n",
    "  Parameters: GRAPH: NetworkX graph.\n",
    "              layout: str: \"spring_layout\" or \"spectral_layout\" (default=\"spring_layout\")\n",
    "              scale: int: multiplier to scale node size by degree.\n",
    "\n",
    "  Returns: plotly plot of network.\n",
    "  \"\"\"\n",
    "  if layout == 'spring_layout':\n",
    "    pos=nx.spring_layout(GRAPH,weight='weight')\n",
    "  elif layout == 'spectral_layout':\n",
    "    pos=nx.spectral_layout(GRAPH,weight='weight')\n",
    "\n",
    "  edge_x = []\n",
    "  edge_y = []\n",
    "  for edge in GRAPH.edges():\n",
    "      x0, y0 = pos[edge[0]]\n",
    "      x1, y1 = pos[edge[1]]\n",
    "      edge_x.append(x0)\n",
    "      edge_x.append(x1)\n",
    "      edge_x.append(None)\n",
    "      edge_y.append(y0)\n",
    "      edge_y.append(y1)\n",
    "      edge_y.append(None)\n",
    "\n",
    "  edge_trace = go.Scatter(\n",
    "      x=edge_x, y=edge_y,\n",
    "      line=dict(width=0.5, color='#888'),\n",
    "      hoverinfo='none',\n",
    "      mode='lines')\n",
    "\n",
    "  node_x = []\n",
    "  node_y = []\n",
    "  for node in GRAPH.nodes():\n",
    "      x, y = pos[node]\n",
    "      node_x.append(x)\n",
    "      node_y.append(y)\n",
    "\n",
    "  node_trace = go.Scatter(\n",
    "      x=node_x, y=node_y,\n",
    "      mode='markers',\n",
    "      hoverinfo='text',\n",
    "      hovertext=[node for node in list(GRAPH.nodes())],\n",
    "      marker=dict(\n",
    "          showscale=False,\n",
    "          # colorscale options\n",
    "          #'Greys' | 'YlGnBu' | 'Greens' | 'YlOrRd' | 'Bluered' | 'RdBu' |\n",
    "          #'Reds' | 'Blues' | 'Picnic' | 'Rainbow' | 'Portland' | 'Jet' |\n",
    "          #'Hot' | 'Blackbody' | 'Earth' | 'Electric' | 'Viridis' |\n",
    "          colorscale='Picnic',\n",
    "          reversescale=True,\n",
    "\n",
    "          color=[node[1]['color'] for node in GRAPH.nodes(data=True)],\n",
    "          size=[GRAPH.degree[node] * scale for node in GRAPH.nodes],\n",
    "          colorbar=dict(\n",
    "              thickness=15,\n",
    "\n",
    "              xanchor='left',\n",
    "              titleside='right'\n",
    "          ),\n",
    "          line_width=2))\n",
    "\n",
    "  fig = go.Figure(data=[edge_trace, node_trace],\n",
    "              layout=go.Layout(\n",
    "                  title=title,\n",
    "                  titlefont_size=16,\n",
    "                  showlegend=False,\n",
    "                  hovermode='closest',\n",
    "\n",
    "                  margin=dict(b=20,l=5,r=5,t=40),\n",
    "                  annotations=[ dict(\n",
    "                      text=\"Python code: <a href='https://plot.ly/ipython-notebooks/network-graphs/'> https://plot.ly/ipython-notebooks/network-graphs/</a>\",\n",
    "                      showarrow=False,\n",
    "                      xref=\"paper\", yref=\"paper\",\n",
    "                      x=0.005, y=-0.002 ) ],\n",
    "                  xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                  yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                  )\n",
    "  plot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hBmH6A26Rzr2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\bertopic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re, urllib, bleach\n",
    "from pandas import DataFrame\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import json, csv, sqlite3\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "#import selenium\n",
    "from natsort import natsorted\n",
    "#from selenium import webdriver\n",
    "from natsort import natsorted\n",
    "import copy\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "smodel = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "def flatten_list(somelist):\n",
    "        if any(isinstance(el, list) for el in somelist) == False:\n",
    "            return somelist\n",
    "        flat_list = list(itertools.chain(*somelist))\n",
    "        return flat_list\n",
    "\n",
    "def citydata_scrape_local(urls):\n",
    "    pages = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            page = urllib.request.urlopen(url)\n",
    "            pages.append(page)\n",
    "        except (IOError, AttributeError):\n",
    "            pass\n",
    "    soups = [bsoup(pages[i]) for i in range(len(pages))]\n",
    "    tables = city_data_tables(soups)\n",
    "\n",
    "    #get timestamps\n",
    "    timestamps = [table.findAll('div',{'class':'normal',}) for table in tables]\n",
    "    datetime = []\n",
    "    for ts in timestamps:\n",
    "        datetime.append(re.findall(r'[0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9],+\\s+[0-9][0-9]:[0-9][0-9]+\\s+['\n",
    "                                        r'A-Z]['\n",
    "                                  r'A-Z]', str(ts)))\n",
    "    dates = []\n",
    "    for date in datetime:\n",
    "        if date == []:\n",
    "            dates.append('')\n",
    "        else:\n",
    "            dates.append(date[0])\n",
    "    #get posts\n",
    "    posts_init = [s.findAll(\"td\", {\"id\": lambda L: L and L.startswith('td_post_')}) for s in tables]\n",
    "    #get post ids\n",
    "    pattern = re.compile('div id=\"[^\"]*\"')\n",
    "    div_ids = [s.findAll(\"td\", {\"id\": lambda L: L and L.startswith('td_post_')}) for s in tables]\n",
    "    ids_init = []\n",
    "    for div in div_ids:\n",
    "        ids_init.append(re.findall('\\d+',str(pattern.findall(str(div)))))\n",
    "\n",
    "    #get quoted post\n",
    "    quoted_init = [post.findAll('td',{'class':'alt2','style':\"border:1px inset\"}) for post in tables]  #quoted json\n",
    "    quoted = []\n",
    "    for qi in quoted_init:\n",
    "        if re.search('Originally Posted', str(qi)):\n",
    "            quoted.append(qi[0])\n",
    "        else:\n",
    "            quoted.append('')\n",
    "    #get quote id\n",
    "    quote_ids = [get_quote_id(quote) for quote in quoted]\n",
    "    qb = []\n",
    "    for quote in quote_ids:\n",
    "        if quote == []:\n",
    "            qb.append('')\n",
    "        else:\n",
    "            qb.append(str(flatten_list(quote)[0]))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['post_id'] = [str(i[0])[1:-1] for i in ids_init]\n",
    "    df['post_body'] = [str(p[0])[1:-1] for p in posts_init]\n",
    "    df['post'] = [normalize_text(str(p)) for p in posts_init]\n",
    "    df['datetime'] = pd.to_datetime(dates)\n",
    "    df['quote_id'] = qb\n",
    "    df['quote_body'] = quoted\n",
    "    df['quote'] = [normalize_text(str(quote)) for quote in quoted]\n",
    "    return df\n",
    "\n",
    "# def cityscrape_from_url_dataframe(urls):\n",
    "\n",
    "#     pages = []\n",
    "#     for url in urls:\n",
    "#          try:\n",
    "#              page = urllib.request.urlopen(url)\n",
    "\n",
    "#              pages.append(page)\n",
    "#          except (IOError, AttributeError):\n",
    "#              pass\n",
    "\n",
    "\n",
    "#     soups = [bsoup(pages[i]) for i in range(len(pages))]\n",
    "\n",
    "#     return soups\n",
    "#     tables = flatten_list([s.findAll('table') for s in soups])\n",
    "    \n",
    "#     #city_data post meta\n",
    "#     ids = city_data_ids(soups)\n",
    "#     dates = city_data_datetime(soups) #datetime json\n",
    "\n",
    "    \n",
    "#     # names = city_data_names(soups) #member json\n",
    "\n",
    "#     #city_data post content\n",
    "#     posts =  city_data_posts(soups)\n",
    "    \n",
    "    \n",
    "\n",
    "#     #cleaned_posts = [re.subn('\\n|\\t', ' ', bleach.clean(posts[i].text, tags=[], attributes=[], styles=[], strip=True))[0].encode('ascii', 'ignore') for i in range(len(posts))]\n",
    "\n",
    "#     quoted_init = [post.findAll('td',{'class':'alt2','style':\"border:1px inset\"}) for post in posts]  #quoted json\n",
    "#     quoted = []\n",
    "#     for qi in quoted_init:\n",
    "#         if re.search('Originally Posted', str(qi)):\n",
    "#             quoted.append(qi)\n",
    "#         else:\n",
    "#             quoted.append([])\n",
    "\n",
    "#     quote_ids = [get_quote_id(quote) for quote in quoted]\n",
    "\n",
    "#     #create dataframe for export\n",
    "#     data = pd.DataFrame()\n",
    "#     data['post_id'] = ids\n",
    "#     #data['quote_id'] = quote_ids\n",
    "#     #data['datetime'] = dates\n",
    "#     #data['raw'] = raws\n",
    "#     #data['quote_body'] = quoted\n",
    "#     #data['cleaned_quotes'] = cleaned_quotes\n",
    "#     #data['post'] = [normalize_text(str(p)) for p in posts]\n",
    "#     data['post_body'] = posts\n",
    "\n",
    "# #    data['links'] = links\n",
    "# #    data['video'] = videos\n",
    "# #    data['images'] = images\n",
    "\n",
    "\n",
    "\n",
    "#     #data.to_csv('city_data0.csv')\n",
    "#     return data\n",
    "\n",
    "def add_cleaned_posts(dataframe):\n",
    "    cposts = dataframe['post_body'].tolist()\n",
    "    cleaned_posts = [bleach.clean(str(remove_table(bsoup(post))), strip=True) for post in cposts]\n",
    "    cp = [re.subn('Quote:','',p)[0] for p in cleaned_posts]\n",
    "    dataframe['cleaned_post'] = cp\n",
    "    return dataframe\n",
    "\n",
    "def get_quote_id(quote):\n",
    "    qi = []\n",
    "    q = str(quote).split()\n",
    "    for i in q:\n",
    "        if re.search('href=', i):\n",
    "            qi.append(i.split('#post')[-1])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    qid  =[]\n",
    "    if len(qi) > 0:\n",
    "        qid.append(re.subn('\"','',qi[0])[0])\n",
    "    else:\n",
    "        qid.append('')\n",
    "    return qid\n",
    "\n",
    "\n",
    "\n",
    "def page_save(urls,t=3):\n",
    "    content = []\n",
    "    browser = webdriver.Firefox()\n",
    "    for url in urls:\n",
    "        try:\n",
    "            browser.get(url)\n",
    "            time.sleep(t)\n",
    "            content.append(browser.page_source)\n",
    "\n",
    "            print(url + ' downloaded!')\n",
    "\n",
    "        except:\n",
    "            print('WebDriverException')\n",
    "\n",
    "    return content\n",
    "\n",
    "def remove_table(tagset):\n",
    "    for t in tagset.findAll('table'):\n",
    "        t.decompose()\n",
    "    return tagset\n",
    "\n",
    "def datetime_post_count_data(dataframe):\n",
    "    \"\"\"takes dataframe from city_data scrape as args\"\"\"\n",
    "\n",
    "    #create new dataframe\n",
    "    df = DataFrame()\n",
    "\n",
    "    #add post count data; each date = 1 post\n",
    "    df['post_count'] = [1 for i in range(len(dataframe))]\n",
    "\n",
    "    # reformat city_data datetime to pandas datetime\n",
    "    df.index = pd.to_datetime(dataframe['datetime'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def city_data_video_src(listofvideos):\n",
    "    \"\"\"helper function that takes lists of iframe links per city_data post; requires that a list of videos per post\n",
    "    be created first\"\"\"\n",
    "    sources = []\n",
    "    if len(listofvideos) > 0:\n",
    "        for v in listofvideos:\n",
    "\n",
    "            sources.append(v['src'])\n",
    "\n",
    "    else:\n",
    "        sources.append([])\n",
    "    return sources\n",
    "\n",
    "def city_data_tables(soups):\n",
    "    # if len(soups) < 3:\n",
    "    #     tables_init = soup.findAll('table', {\"id\": lambda L: L and L.startswith('post')})\n",
    "    # else:\n",
    "    tables_init = [s.findAll(\"table\", {\"id\": lambda L: L and L.startswith('post')}) for s in soups]\n",
    "    tables = []\n",
    "    for table in tables_init:\n",
    "        if table not in tables:\n",
    "            tables.append(table)\n",
    "\n",
    "    return flatten_list(tables)\n",
    "\n",
    "def city_data_names(soups):\n",
    "    # if len(soups) < 3:\n",
    "    #     tables_init = soup.findAll('table', {\"id\": lambda L: L and L.startswith('post')})\n",
    "    # else:\n",
    "\n",
    "    names_init = [s.findAll(\"a\", {\"class\": \"bigusername\"}) for s in soups]\n",
    "    names = [bleach.clean(n, tags=[], attributes=[], styles=[], strip=True) for n in flatten_list(names_init)]\n",
    "\n",
    "\n",
    "    return names\n",
    "\n",
    "def city_data_ids(soups):\n",
    "    pattern = re.compile('div id=\"[^\"]*\"')\n",
    "    # if len(soups) < 3:\n",
    "    #     div_ids = soups.findAll(\"div\", {\"id\": lambda L: L and L.startswith('post_message_')})\n",
    "    # else:\n",
    "    div_ids = [s.findAll(\"td\", {\"id\": lambda L: L and L.startswith('td_post_')}) for s in soups]\n",
    "    ids_init = []\n",
    "    for div in div_ids:\n",
    "        ids_init.append(re.findall('\\d+',str(pattern.findall(str(div)))))\n",
    "\n",
    "    ids = []\n",
    "    for id in ids_init:\n",
    "        if id not in ids:\n",
    "            ids.append(id)\n",
    "\n",
    "    return flatten_list(ids)\n",
    "\n",
    "\n",
    "def city_data_posts(soups):\n",
    "    # if len(soups) < 3:\n",
    "    #     posts_init = soups.findAll('div', {\"id\": lambda L: L and L.startswith('post_message_')})\n",
    "    # else:\n",
    "    #posts_init = [s.findAll(\"div\", {\"id\": lambda L: L and L.startswith('post_message_')}) for s in soups] # collect\n",
    "    posts_init = [s.findAll(\"td\", {\"id\": lambda L: L and L.startswith('td_post_')}) for s in soups] # collect\n",
    "\n",
    "    #beware of duplicates!\n",
    "    posts = []  # post_content json\n",
    "    for p in posts_init:\n",
    "        if p not in posts:\n",
    "            posts.append(p)\n",
    "\n",
    "    return flatten_list(posts)\n",
    "\n",
    "def city_data_datetime(soups):\n",
    "    \n",
    "    # if len(soups) < 3:\n",
    "    #     timestamps = soups.findAll('td',{'class':'thead'})\n",
    "    # else:\n",
    "    #tds = [s.findAll('td',{'class':'thead'}) for s in soups]\n",
    "    #posts = city_data_posts(soups)\n",
    "    timestamps = [s.findAll('div',{'class':'normal','style': 'color:#A2B3D0;'}) for s in soups]\n",
    "    datetime_init = []\n",
    "\n",
    "    for ts in timestamps:\n",
    "        datetime_init.append(re.findall(r'[0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9],+\\s+[0-9][0-9]:[0-9][0-9]+\\s+['\n",
    "                                        r'A-Z]['\n",
    "                                  r'A-Z]', str(ts)))\n",
    "    datetime = []\n",
    "    for date in datetime_init:\n",
    "        if date not in datetime:\n",
    "            datetime.append(date)\n",
    "            \n",
    "\n",
    "    return datetime\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "def get_topical_coherence(texts,topics, metric='u_mass'):\n",
    "    if type(texts[0]) == str:\n",
    "        #train dictionary on tokenized documents (corpus)\n",
    "        doc_tokens = [text_tokenize(text) for text in texts]\n",
    "        phrases = Phrases(doc_tokens, min_count=1, threshold=1)\n",
    "        bigrams = phrases[doc_tokens]\n",
    "        dct = Dictionary(bigrams) \n",
    "        \n",
    "        \n",
    "        #topics = list of topical words\n",
    "        #texts = tokenized documents\n",
    "        #corpus = doc2bow\n",
    "        #dictionary = gensim dictionary\n",
    "        cm = CoherenceModel(topics=topics,texts=texts, corpus=[dct.doc2bow(c) for c in doc_tokens], dictionary=dct, coherence=metric)\n",
    "        return cm.get_coherence()\n",
    "    elif type(texts[0]) == list:\n",
    "        doc_tokens = texts\n",
    "        phrases = Phrases(doc_tokens, min_count=1, threshold=1)\n",
    "        bigrams = phrases[doc_tokens]\n",
    "        dct = Dictionary(bigrams) \n",
    "        \n",
    "        \n",
    "        #topics = list of topical words\n",
    "        #texts = tokenized documents\n",
    "        #corpus = doc2bow\n",
    "        #dictionary = gensim dictionary\n",
    "        cm = CoherenceModel(topics=topics,texts=texts, corpus=[dct.doc2bow(c) for c in doc_tokens], dictionary=dct, coherence=metric)\n",
    "        return cm.get_coherence()\n",
    "        \n",
    "\n",
    "def get_topical_terms(dataframe, term_count=25):\n",
    "    topics = []\n",
    "    for i in range(len(dataframe.columns)-1):\n",
    "        \n",
    "        terms = dataframe.sort_values(by='coef_'+str(i),ascending=False)[:term_count]['term'].tolist()\n",
    "        topics.append(terms)\n",
    "    filtered = []\n",
    "    for topic in topics:\n",
    "        t = []\n",
    "        for term in topic:\n",
    "            if len(term.split()) > 1:\n",
    "                t.append('_'.join(term.split()))\n",
    "            else:\n",
    "                t.append(term)\n",
    "        filtered.append(t)\n",
    "    return filtered\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def proportion_unique_words(topics, topk=10):\n",
    "    \"\"\"\n",
    "    compute the proportion of unique words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    topk: top k words on which the topic diversity will be computed\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than '+str(topk))\n",
    "    else:\n",
    "        unique_words = set()\n",
    "        for topic in topics:\n",
    "            unique_words = unique_words.union(set(topic[:topk]))\n",
    "        puw = len(unique_words) / (topk * len(topics))\n",
    "        return puw\n",
    "\n",
    "\n",
    "def irbo(topics, weight=0.9, topk=10):\n",
    "    \"\"\"\n",
    "    compute the inverted rank-biased overlap\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    weight: p (float), default 1.0: Weight of each\n",
    "        agreement at depth d:p**(d-1). When set\n",
    "        to 1.0, there is no weight, the rbo returns\n",
    "        to average overlap.\n",
    "    topk: top k words on which the topic diversity\n",
    "          will be computed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    irbo : score of the rank biased overlap over the topics\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than topk')\n",
    "    else:\n",
    "        collect = []\n",
    "        for list1, list2 in combinations(topics, 2):\n",
    "            word2index = get_word2index(list1, list2)\n",
    "            indexed_list1 = [word2index[word] for word in list1]\n",
    "            indexed_list2 = [word2index[word] for word in list2]\n",
    "            rbo_val = rbo(indexed_list1[:topk], indexed_list2[:topk], p=weight)[2]\n",
    "            collect.append(rbo_val)\n",
    "        return 1 - np.mean(collect)\n",
    "\n",
    "\n",
    "def word_embedding_irbo(topics, word_embedding_model, weight=0.9, topk=10):\n",
    "    '''\n",
    "    compute the word embedding-based inverted rank-biased overlap\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    weight: p (float), default 1.0: Weight of each agreement at depth d:\n",
    "    p**(d-1). When set to 1.0, there is no weight, the rbo returns to average overlap.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    weirbo: word embedding-based inverted rank_biased_overlap over the topics\n",
    "    '''\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than topk')\n",
    "    else:\n",
    "        collect = []\n",
    "        for list1, list2 in combinations(topics, 2):\n",
    "            word2index = get_word2index(list1, list2)\n",
    "            index2word = {v: k for k, v in word2index.items()}\n",
    "            indexed_list1 = [word2index[word] for word in list1]\n",
    "            indexed_list2 = [word2index[word] for word in list2]\n",
    "            rbo_val = word_embeddings_rbo(indexed_list1[:topk], indexed_list2[:topk], p=weight,\n",
    "                                          index2word=index2word, word2vec=word_embedding_model)[2]\n",
    "            collect.append(rbo_val)\n",
    "        return 1 - np.mean(collect)\n",
    "\n",
    "\n",
    "def pairwise_jaccard_diversity(topics, topk=10):\n",
    "    '''\n",
    "    compute the average pairwise jaccard distance between the topics \n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    topk: top k words on which the topic diversity\n",
    "          will be computed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pjd: average pairwise jaccard distance\n",
    "    '''\n",
    "    dist = 0\n",
    "    count = 0\n",
    "    for list1, list2 in combinations(topics, 2):\n",
    "        js = 1 - len(set(list1).intersection(set(list2)))/len(set(list1).union(set(list2)))\n",
    "        dist = dist + js\n",
    "        count = count + 1\n",
    "    return dist/count\n",
    "\n",
    "\n",
    "def pairwise_word_embedding_distance(topics, word_embedding_model, topk=10):\n",
    "    \"\"\"\n",
    "    :param topk: how many most likely words to consider in the evaluation\n",
    "    :return: topic coherence computed on the word embeddings similarities\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than topk')\n",
    "    else:\n",
    "        count = 0\n",
    "        sum_dist = 0\n",
    "        for list1, list2 in combinations(topics, 2):\n",
    "            count = count+1\n",
    "            word_counts = 0\n",
    "            dist = 0\n",
    "            for word1 in list1[:topk]:\n",
    "                for word2 in list2[:topk]:\n",
    "                    dist = dist + distance.cosine(word_embedding_model.wv[word1], word_embedding_model.wv[word2])\n",
    "                    word_counts = word_counts + 1\n",
    "\n",
    "            dist = dist/word_counts\n",
    "            sum_dist = sum_dist + dist\n",
    "        return sum_dist/count\n",
    "\n",
    "\n",
    "def centroid_distance(topics, word_embedding_model, topk=10):\n",
    "    \"\"\"\n",
    "    :param topk: how many most likely words to consider in the evaluation\n",
    "    :return: topic coherence computed on the word embeddings similarities\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than topk')\n",
    "    else:\n",
    "        count = 0\n",
    "        for list1, list2 in combinations(topics, 2):\n",
    "            count = count + 1\n",
    "            centroid1 = np.zeros(word_embedding_model.vector_size)\n",
    "            centroid2 = np.zeros(word_embedding_model.vector_size)\n",
    "            for word1 in list1[:topk]:\n",
    "                centroid1 = centroid1 + word_embedding_model[word1]\n",
    "            for word2 in list2[:topk]:\n",
    "                centroid2 = centroid2 + word_embedding_model[word2]\n",
    "            centroid1 = centroid1 / len(list1[:topk])\n",
    "            centroid2 = centroid2 / len(list2[:topk])\n",
    "        return distance.cosine(centroid1, centroid2)\n",
    "\n",
    "\n",
    "def get_word2index(list1, list2):\n",
    "    words = set(list1)\n",
    "    words = words.union(set(list2))\n",
    "    word2index = {w: i for i, w in enumerate(words)}\n",
    "    return word2index\n",
    "\"\"\"Rank-biased overlap, a ragged sorted list similarity measure.\n",
    "See http://doi.acm.org/10.1145/1852102.1852106 for details. All functions\n",
    "directly corresponding to concepts from the paper are named so that they can be\n",
    "clearly cross-identified.\n",
    "The definition of overlap has been modified to account for ties. Without this,\n",
    "results for lists with tied items were being inflated. The modification itself\n",
    "is not mentioned in the paper but seems to be reasonable, see function\n",
    "``overlap()``. Places in the code which diverge from the spec in the paper\n",
    "because of this are highlighted with comments.\n",
    "The two main functions for performing an RBO analysis are ``rbo()`` and\n",
    "``rbo_dict()``; see their respective docstrings for how to use them.\n",
    "The following doctest just checks that equivalent specifications of a\n",
    "problem yield the same result using both functions:\n",
    "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
    "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
    "    >>> ans_rbo = _round(rbo(lst1, lst2, p=.9))\n",
    "    >>> dct1 = dict(a=1, b=2, c=1, d=3)\n",
    "    >>> dct2 = dict(a=1, b=2, c=2, d=3)\n",
    "    >>> ans_rbo_dict = _round(rbo_dict(dct1, dct2, p=.9, sort_ascending=True))\n",
    "    >>> ans_rbo == ans_rbo_dict\n",
    "    True\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from bisect import bisect_left\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "RBO = namedtuple(\"RBO\", \"min res ext\")\n",
    "RBO.__doc__ += \": Result of full RBO analysis\"\n",
    "RBO.min.__doc__ = \"Lower bound estimate\"\n",
    "RBO.res.__doc__ = \"Residual corresponding to min; min + res is an upper bound estimate\"\n",
    "RBO.ext.__doc__ = \"Extrapolated point estimate\"\n",
    "\n",
    "\n",
    "def _round(obj):\n",
    "    if isinstance(obj, RBO):\n",
    "        return RBO(_round(obj.min), _round(obj.res), _round(obj.ext))\n",
    "    else:\n",
    "        return round(obj, 3)\n",
    "\n",
    "\n",
    "def set_at_depth(lst, depth):\n",
    "    ans = set()\n",
    "    for v in lst[:depth]:\n",
    "        if isinstance(v, set):\n",
    "            ans.update(v)\n",
    "        else:\n",
    "            ans.add(v)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def raw_overlap(list1, list2, depth):\n",
    "    \"\"\"Overlap as defined in the article.\n",
    "    \"\"\"\n",
    "    set1, set2 = set_at_depth(list1, depth), set_at_depth(list2, depth)\n",
    "    return len(set1.intersection(set2)), len(set1), len(set2)\n",
    "\n",
    "\n",
    "def overlap(list1, list2, depth):\n",
    "    \"\"\"Overlap which accounts for possible ties.\n",
    "    This isn't mentioned in the paper but should be used in the ``rbo*()``\n",
    "    functions below, otherwise overlap at a given depth might be > depth which\n",
    "    inflates the result.\n",
    "    There are no guidelines in the paper as to what's a good way to calculate\n",
    "    this, but a good guess is agreement scaled by the minimum between the\n",
    "    requested depth and the lengths of the considered lists (overlap shouldn't\n",
    "    be larger than the number of ranks in the shorter list, otherwise results\n",
    "    are conspicuously wrong when the lists are of unequal lengths -- rbo_ext is\n",
    "    not between rbo_min and rbo_min + rbo_res.\n",
    "    >>> overlap(\"abcd\", \"abcd\", 3)\n",
    "    3.0\n",
    "    >>> overlap(\"abcd\", \"abcd\", 5)\n",
    "    4.0\n",
    "    >>> overlap([\"a\", {\"b\", \"c\"}, \"d\"], [\"a\", {\"b\", \"c\"}, \"d\"], 2)\n",
    "    2.0\n",
    "    >>> overlap([\"a\", {\"b\", \"c\"}, \"d\"], [\"a\", {\"b\", \"c\"}, \"d\"], 3)\n",
    "    3.0\n",
    "    \"\"\"\n",
    "    ov = agreement(list1, list2, depth) * min(depth, len(list1), len(list2))\n",
    "    return ov\n",
    "    # NOTE: comment the preceding and uncomment the following line if you want\n",
    "    # to stick to the algorithm as defined by the paper\n",
    "    # return raw_overlap(list1, list2, depth)[0]\n",
    "\n",
    "\n",
    "def agreement(list1, list2, depth):\n",
    "    \"\"\"Proportion of shared values between two sorted lists at given depth.\n",
    "    >>> _round(agreement(\"abcde\", \"abdcf\", 1))\n",
    "    1.0\n",
    "    >>> _round(agreement(\"abcde\", \"abdcf\", 3))\n",
    "    0.667\n",
    "    >>> _round(agreement(\"abcde\", \"abdcf\", 4))\n",
    "    1.0\n",
    "    >>> _round(agreement(\"abcde\", \"abdcf\", 5))\n",
    "    0.8\n",
    "    >>> _round(agreement([{1, 2}, 3], [1, {2, 3}], 1))\n",
    "    0.667\n",
    "    >>> _round(agreement([{1, 2}, 3], [1, {2, 3}], 2))\n",
    "    1.0\n",
    "    \"\"\"\n",
    "    len_intersection, len_set1, len_set2 = raw_overlap(list1, list2, depth)\n",
    "    return 2 * len_intersection / (len_set1 + len_set2)\n",
    "\n",
    "\n",
    "def cumulative_agreement(list1, list2, depth):\n",
    "    return (agreement(list1, list2, d) for d in range(1, depth + 1))\n",
    "\n",
    "\n",
    "def average_overlap(list1, list2, depth=None):\n",
    "    \"\"\"Calculate average overlap between ``list1`` and ``list2``.\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 1))\n",
    "    0.0\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 2))\n",
    "    0.0\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 3))\n",
    "    0.222\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 4))\n",
    "    0.292\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 5))\n",
    "    0.313\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 6))\n",
    "    0.317\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 7))\n",
    "    0.312\n",
    "    \"\"\"\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    return sum(cumulative_agreement(list1, list2, depth)) / depth\n",
    "\n",
    "\n",
    "def rbo_at_k(list1, list2, p, depth=None):\n",
    "    # ``p**d`` here instead of ``p**(d - 1)`` because enumerate starts at\n",
    "    # 0\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    d_a = enumerate(cumulative_agreement(list1, list2, depth))\n",
    "    return (1 - p) * sum(p ** d * a for (d, a) in d_a)\n",
    "\n",
    "\n",
    "def rbo_min(list1, list2, p, depth=None):\n",
    "    \"\"\"Tight lower bound on RBO.\n",
    "    See equation (11) in paper.\n",
    "    >>> _round(rbo_min(\"abcdefg\", \"abcdefg\", .9))\n",
    "    0.767\n",
    "    >>> _round(rbo_min(\"abcdefgh\", \"abcdefg\", .9))\n",
    "    0.767\n",
    "    \"\"\"\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    x_k = overlap(list1, list2, depth)\n",
    "    log_term = x_k * math.log(1 - p)\n",
    "    sum_term = sum(\n",
    "        p ** d / d * (overlap(list1, list2, d) - x_k) for d in range(1, depth + 1)\n",
    "    )\n",
    "    return (1 - p) / p * (sum_term - log_term)\n",
    "\n",
    "\n",
    "def rbo_res(list1, list2, p):\n",
    "    \"\"\"Upper bound on residual overlap beyond evaluated depth.\n",
    "    See equation (30) in paper.\n",
    "    NOTE: The doctests weren't verified against manual computations but seem\n",
    "    plausible. In particular, for identical lists, ``rbo_min()`` and\n",
    "    ``rbo_res()`` should add up to 1, which is the case.\n",
    "    >>> _round(rbo_res(\"abcdefg\", \"abcdefg\", .9))\n",
    "    0.233\n",
    "    >>> _round(rbo_res(\"abcdefg\", \"abcdefghijklmnopqrstuvwxyz\", .9))\n",
    "    0.239\n",
    "    \"\"\"\n",
    "    S, L = sorted((list1, list2), key=len)\n",
    "    s, l = len(S), len(L)\n",
    "    x_l = overlap(list1, list2, l)\n",
    "    # since overlap(...) can be fractional in the general case of ties and f\n",
    "    # must be an integer --> math.ceil()\n",
    "    f = int(math.ceil(l + s - x_l))\n",
    "    # upper bound of range() is non-inclusive, therefore + 1 is needed\n",
    "    term1 = s * sum(p ** d / d for d in range(s + 1, f + 1))\n",
    "    term2 = l * sum(p ** d / d for d in range(l + 1, f + 1))\n",
    "    term3 = x_l * (math.log(1 / (1 - p)) - sum(p ** d / d for d in range(1, f + 1)))\n",
    "    return p ** s + p ** l - p ** f - (1 - p) / p * (term1 + term2 + term3)\n",
    "\n",
    "\n",
    "def rbo_ext(list1, list2, p):\n",
    "    \"\"\"RBO point estimate based on extrapolating observed overlap.\n",
    "    See equation (32) in paper.\n",
    "    NOTE: The doctests weren't verified against manual computations but seem\n",
    "    plausible.\n",
    "    >>> _round(rbo_ext(\"abcdefg\", \"abcdefg\", .9))\n",
    "    1.0\n",
    "    >>> _round(rbo_ext(\"abcdefg\", \"bacdefg\", .9))\n",
    "    0.9\n",
    "    \"\"\"\n",
    "    S, L = sorted((list1, list2), key=len)\n",
    "    s, l = len(S), len(L)\n",
    "    x_l = overlap(list1, list2, l)\n",
    "    x_s = overlap(list1, list2, s)\n",
    "    # the paper says overlap(..., d) / d, but it should be replaced by\n",
    "    # agreement(..., d) defined as per equation (28) so that ties are handled\n",
    "    # properly (otherwise values > 1 will be returned)\n",
    "    # sum1 = sum(p**d * overlap(list1, list2, d)[0] / d for d in range(1, l + 1))\n",
    "    sum1 = sum(p ** d * agreement(list1, list2, d) for d in range(1, l + 1))\n",
    "    sum2 = sum(p ** d * x_s * (d - s) / s / d for d in range(s + 1, l + 1))\n",
    "    term1 = (1 - p) / p * (sum1 + sum2)\n",
    "    term2 = p ** l * ((x_l - x_s) / l + x_s / s)\n",
    "    return term1 + term2\n",
    "\n",
    "\n",
    "def rbo(list1, list2, p):\n",
    "    \"\"\"Complete RBO analysis (lower bound, residual, point estimate).\n",
    "    ``list`` arguments should be already correctly sorted iterables and each\n",
    "    item should either be an atomic value or a set of values tied for that\n",
    "    rank. ``p`` is the probability of looking for overlap at rank k + 1 after\n",
    "    having examined rank k.\n",
    "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
    "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
    "    >>> _round(rbo(lst1, lst2, p=.9))\n",
    "    RBO(min=0.489, res=0.477, ext=0.967)\n",
    "    \"\"\"\n",
    "    if not 0 <= p <= 1:\n",
    "        raise ValueError(\"The ``p`` parameter must be between 0 and 1.\")\n",
    "    args = (list1, list2, p)\n",
    "    return RBO(rbo_min(*args), rbo_res(*args), rbo_ext(*args))\n",
    "\n",
    "\n",
    "def sort_dict(dct, *, ascending=False):\n",
    "    \"\"\"Sort keys in ``dct`` according to their corresponding values.\n",
    "    Sorts in descending order by default, because the values are\n",
    "    typically scores, i.e. the higher the better. Specify\n",
    "    ``ascending=True`` if the values are ranks, or some sort of score\n",
    "    where lower values are better.\n",
    "    Ties are handled by creating sets of tied keys at the given position\n",
    "    in the sorted list.\n",
    "    >>> dct = dict(a=1, b=2, c=1, d=3)\n",
    "    >>> list(sort_dict(dct)) == ['d', 'b', {'a', 'c'}]\n",
    "    True\n",
    "    >>> list(sort_dict(dct, ascending=True)) == [{'a', 'c'}, 'b', 'd']\n",
    "    True\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    items = []\n",
    "    # items should be unique, scores don't have to\n",
    "    for item, score in dct.items():\n",
    "        if not ascending:\n",
    "            score *= -1\n",
    "        i = bisect_left(scores, score)\n",
    "        if i == len(scores):\n",
    "            scores.append(score)\n",
    "            items.append(item)\n",
    "        elif scores[i] == score:\n",
    "            existing_item = items[i]\n",
    "            if isinstance(existing_item, set):\n",
    "                existing_item.add(item)\n",
    "            else:\n",
    "                items[i] = {existing_item, item}\n",
    "        else:\n",
    "            scores.insert(i, score)\n",
    "            items.insert(i, item)\n",
    "    return items\n",
    "\n",
    "\n",
    "def rbo_dict(dict1, dict2, p, *, sort_ascending=False):\n",
    "    \"\"\"Wrapper around ``rbo()`` for dict input.\n",
    "    Each dict maps items to be sorted to the score according to which\n",
    "    they should be sorted. The RBO analysis is then performed on the\n",
    "    resulting sorted lists.\n",
    "    The sort is descending by default, because scores are typically the\n",
    "    higher the better, but this can be overridden by specifying\n",
    "    ``sort_ascending=True``.\n",
    "    >>> dct1 = dict(a=1, b=2, c=1, d=3)\n",
    "    >>> dct2 = dict(a=1, b=2, c=2, d=3)\n",
    "    >>> _round(rbo_dict(dct1, dct2, p=.9, sort_ascending=True))\n",
    "    RBO(min=0.489, res=0.477, ext=0.967)\n",
    "    \"\"\"\n",
    "    list1, list2 = (\n",
    "        sort_dict(dict1, ascending=sort_ascending),\n",
    "        sort_dict(dict2, ascending=sort_ascending),\n",
    "    )\n",
    "    return rbo(list1, list2, p)\n",
    "\n",
    "import math\n",
    "from bisect import bisect_left\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "RBO = namedtuple(\"RBO\", \"min res ext\")\n",
    "RBO.__doc__ += \": Result of full RBO analysis\"\n",
    "RBO.min.__doc__ = \"Lower bound estimate\"\n",
    "RBO.res.__doc__ = \"Residual corresponding to min; min + res is an upper bound estimate\"\n",
    "RBO.ext.__doc__ = \"Extrapolated point estimate\"\n",
    "\n",
    "def _round(obj):\n",
    "    if isinstance(obj, RBO):\n",
    "        return RBO(_round(obj.min), _round(obj.res), _round(obj.ext))\n",
    "    else:\n",
    "        return round(obj, 3)\n",
    "\n",
    "\n",
    "def set_at_depth(lst, depth):\n",
    "    ans = set()\n",
    "    for v in lst[:depth]:\n",
    "        if isinstance(v, set):\n",
    "            ans.update(v)\n",
    "        else:\n",
    "            ans.add(v)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def embeddings_overlap(list1, list2, depth, index2word, word2vec):\n",
    "    #set1, set2 = set_at_depth(list1, depth), set_at_depth(list2, depth)\n",
    "    #return len(set1.intersection(set2)), len(set1), len(set2)\n",
    "\n",
    "    set1, set2 = set_at_depth(list1, depth), set_at_depth(list2, depth)\n",
    "    word_list1 = [index2word[index] for index in list1]\n",
    "    word_list2 = [index2word[index] for index in list2]\n",
    "\n",
    "    similarities = {}\n",
    "    for w1 in word_list1[:depth]:\n",
    "        for w2 in word_list2[:depth]:\n",
    "            similarities[(w1,w2)] = word2vec.similarity(w1, w2)\n",
    "\n",
    "    similarities = OrderedDict(sorted(similarities.items(), key=lambda x: -x[1]))\n",
    "\n",
    "    e_ov = 0\n",
    "    key_list = list(similarities.keys())\n",
    "    for k in key_list:\n",
    "        if k in similarities.keys():\n",
    "            #print(k, similarities[k])\n",
    "            e_ov = e_ov + similarities[k]\n",
    "            similarities = {save_k: v for save_k, v in similarities.items()\n",
    "                            if save_k[0] != k[0] and save_k[1] != k[1]}\n",
    "    #e_ov = 1\n",
    "    #print(\"****\")\n",
    "    return e_ov, len(set1), len(set2)\n",
    "\n",
    "\n",
    "def overlap(list1, list2, depth, index2word, word2vec):\n",
    "    #return agreement(list1, list2, depth) * min(depth, len(list1), len(list2))\n",
    "    # NOTE: comment the preceding and uncomment the following line if you want\n",
    "    # to stick to the algorithm as defined by the paper\n",
    "    ov = embeddings_overlap(list1, list2, depth, index2word, word2vec)[0]\n",
    "    return ov\n",
    "\n",
    "\n",
    "def agreement(list1, list2, depth, index2word, word2vec):\n",
    "    \"\"\"Proportion of shared values between two sorted lists at given depth.\"\"\"\n",
    "    len_intersection, len_set1, len_set2 = embeddings_overlap(list1, list2, depth, index2word, word2vec)\n",
    "    return 2 * len_intersection / (len_set1 + len_set2)\n",
    "\n",
    "\n",
    "def cumulative_agreement(list1, list2, depth, index2word, word2vec):\n",
    "    return (agreement(list1, list2, d, index2word, word2vec) for d in range(1, depth + 1))\n",
    "\n",
    "\n",
    "def average_overlap(list1, list2, index2word, word2vec, depth=None):\n",
    "    \"\"\"Calculate average overlap between ``list1`` and ``list2``.\n",
    "    \"\"\"\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    return sum(cumulative_agreement(list1, list2, depth, index2word=index2word, word2vec=word2vec)) / depth\n",
    "\n",
    "\n",
    "def rbo_at_k(list1, list2, p, index2word, word2vec, depth=None):\n",
    "    # ``p**d`` here instead of ``p**(d - 1)`` because enumerate starts at\n",
    "    # 0\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    d_a = enumerate(cumulative_agreement(list1, list2, depth, index2word=index2word, word2vec=word2vec))\n",
    "    return (1 - p) * sum(p ** d * a for (d, a) in d_a)\n",
    "\n",
    "\n",
    "def rbo_min(list1, list2, p, index2word, word2vec, depth=None):\n",
    "    \"\"\"Tight lower bound on RBO.\n",
    "    See equation (11) in paper.\n",
    "    \"\"\"\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    x_k = overlap(list1, list2, depth, index2word, word2vec)\n",
    "    log_term = x_k * math.log(1 - p)\n",
    "    sum_term = sum(\n",
    "        p ** d / d * (overlap(list1, list2, d, index2word, word2vec=word2vec) - x_k) for d in range(1, depth + 1)\n",
    "    )\n",
    "    return (1 - p) / p * (sum_term - log_term)\n",
    "\n",
    "\n",
    "def rbo_res(list1, list2, p, index2word, word2vec):\n",
    "    \"\"\"Upper bound on residual overlap beyond evaluated depth.\n",
    "    See equation (30) in paper.\n",
    "    NOTE: The doctests weren't verified against manual computations but seem\n",
    "    plausible. In particular, for identical lists, ``rbo_min()`` and\n",
    "    ``rbo_res()`` should add up to 1, which is the case.\n",
    "    \"\"\"\n",
    "    S, L = sorted((list1, list2), key=len)\n",
    "    s, l = len(S), len(L)\n",
    "    x_l = overlap(list1, list2, l, index2word, word2vec)\n",
    "    # since overlap(...) can be fractional in the general case of ties and f\n",
    "    # must be an integer --> math.ceil()\n",
    "    f = int(math.ceil(l + s - x_l))\n",
    "    # upper bound of range() is non-inclusive, therefore + 1 is needed\n",
    "    term1 = s * sum(p ** d / d for d in range(s + 1, f + 1))\n",
    "    term2 = l * sum(p ** d / d for d in range(l + 1, f + 1))\n",
    "    term3 = x_l * (math.log(1 / (1 - p)) - sum(p ** d / d for d in range(1, f + 1)))\n",
    "    return p ** s + p ** l - p ** f - (1 - p) / p * (term1 + term2 + term3)\n",
    "\n",
    "\n",
    "def rbo_ext(list1, list2, p, index2word, word2vec):\n",
    "    \"\"\"RBO point estimate based on extrapolating observed overlap.\n",
    "    See equation (32) in paper.\n",
    "    NOTE: The doctests weren't verified against manual computations but seem\n",
    "    plausible.\n",
    "    >>> _round(rbo_ext(\"abcdefg\", \"abcdefg\", .9))\n",
    "    1.0\n",
    "    >>> _round(rbo_ext(\"abcdefg\", \"bacdefg\", .9))\n",
    "    0.9\n",
    "    \"\"\"\n",
    "    S, L = sorted((list1, list2), key=len)\n",
    "    s, l = len(S), len(L)\n",
    "    x_l = overlap(list1, list2, l, index2word, word2vec)\n",
    "    x_s = overlap(list1, list2, s, index2word, word2vec)\n",
    "    # the paper says overlap(..., d) / d, but it should be replaced by\n",
    "    # agreement(..., d) defined as per equation (28) so that ties are handled\n",
    "    # properly (otherwise values > 1 will be returned)\n",
    "    # sum1 = sum(p**d * overlap(list1, list2, d)[0] / d for d in range(1, l + 1))\n",
    "    sum1 = sum(p ** d * agreement(list1, list2, d, index2word=index2word, word2vec=word2vec)\n",
    "               for d in range(1, l + 1))\n",
    "    sum2 = sum(p ** d * x_s * (d - s) / s / d for d in range(s + 1, l + 1))\n",
    "    term1 = (1 - p) / p * (sum1 + sum2)\n",
    "    term2 = p ** l * ((x_l - x_s) / l + x_s / s)\n",
    "    return term1 + term2\n",
    "\n",
    "\n",
    "def word_embeddings_rbo(list1, list2, p, index2word, word2vec):\n",
    "    \"\"\"Complete RBO analysis (lower bound, residual, point estimate).\n",
    "    ``list`` arguments should be already correctly sorted iterables and each\n",
    "    item should either be an atomic value or a set of values tied for that\n",
    "    rank. ``p`` is the probability of looking for overlap at rank k + 1 after\n",
    "    having examined rank k.\n",
    "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
    "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
    "    >>> _round(rbo(lst1, lst2, p=.9))\n",
    "    RBO(min=0.489, res=0.477, ext=0.967)\n",
    "    \"\"\"\n",
    "    if not 0 <= p <= 1:\n",
    "        raise ValueError(\"The ``p`` parameter must be between 0 and 1.\")\n",
    "    args = (list1, list2, p, index2word, word2vec)\n",
    "\n",
    "    return RBO(rbo_min(*args), rbo_res(*args), rbo_ext(*args))\n",
    "\n",
    "\n",
    "def sort_dict(dct, *, ascending=False):\n",
    "    \"\"\"Sort keys in ``dct`` according to their corresponding values.\n",
    "    Sorts in descending order by default, because the values are\n",
    "    typically scores, i.e. the higher the better. Specify\n",
    "    ``ascending=True`` if the values are ranks, or some sort of score\n",
    "    where lower values are better.\n",
    "    Ties are handled by creating sets of tied keys at the given position\n",
    "    in the sorted list.\n",
    "    >>> dct = dict(a=1, b=2, c=1, d=3)\n",
    "    >>> list(sort_dict(dct)) == ['d', 'b', {'a', 'c'}]\n",
    "    True\n",
    "    >>> list(sort_dict(dct, ascending=True)) == [{'a', 'c'}, 'b', 'd']\n",
    "    True\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    items = []\n",
    "    # items should be unique, scores don't have to\n",
    "    for item, score in dct.items():\n",
    "        if not ascending:\n",
    "            score *= -1\n",
    "        i = bisect_left(scores, score)\n",
    "        if i == len(scores):\n",
    "            scores.append(score)\n",
    "            items.append(item)\n",
    "        elif scores[i] == score:\n",
    "            existing_item = items[i]\n",
    "            if isinstance(existing_item, set):\n",
    "                existing_item.add(item)\n",
    "            else:\n",
    "                items[i] = {existing_item, item}\n",
    "        else:\n",
    "            scores.insert(i, score)\n",
    "            items.insert(i, item)\n",
    "    return items\n",
    "\n",
    "\n",
    "def rbo_dict(dict1, dict2, p, index2word, word2vec, *, sort_ascending=False):\n",
    "    \"\"\"Wrapper around ``rbo()`` for dict input.\n",
    "    Each dict maps items to be sorted to the score according to which\n",
    "    they should be sorted. The RBO analysis is then performed on the\n",
    "    resulting sorted lists.\n",
    "    The sort is descending by default, because scores are typically the\n",
    "    higher the better, but this can be overridden by specifying\n",
    "    ``sort_ascending=True``.\n",
    "    \"\"\"\n",
    "    list1, list2 = (\n",
    "        sort_dict(dict1, ascending=sort_ascending),\n",
    "        sort_dict(dict2, ascending=sort_ascending),\n",
    "    )\n",
    "    return word_embeddings_rbo(list1, list2, p, index2word, word2vec)\n",
    "\n",
    "def get_diversity_scores(topics, model, topn=10, topic_type=''):\n",
    "    df = pd.DataFrame({\"puw:\":proportion_unique_words(topics, topk=topn),\n",
    "\"jd:\": pairwise_jaccard_diversity(topics, topk=topn),\n",
    "\"we-pd:\": pairwise_word_embedding_distance(topics, model, topk=topn),\n",
    "\"we-cd:\": centroid_distance(topics, model.wv, topk=topn),\n",
    "\"we-irbo p=0.5:\":word_embedding_irbo(topics,model.wv, weight=0.5, topk=topn),\n",
    "\"we-irbo p=0.9:\":word_embedding_irbo(topics,model.wv, weight=0.9, topk=topn)},index=[topic_type])\n",
    "    return df\n",
    "\n",
    "def get_coherence_diversity_scores(texts, topics, model, topn=10, topic_type='',metric='u_mass'):\n",
    "    \n",
    "    df = pd.DataFrame({'u_mass_coherence':get_topical_coherence(texts,topics,metric=metric),\n",
    "        \"puw:\":proportion_unique_words(topics, topk=topn),\n",
    "\"jd:\": pairwise_jaccard_diversity(topics, topk=topn),\n",
    "\"we-pd:\": pairwise_word_embedding_distance(topics, model, topk=topn),\n",
    "\"we-cd:\": centroid_distance(topics, model.wv, topk=topn),\n",
    "\"we-irbo p=0.5:\":word_embedding_irbo(topics,model.wv, weight=0.5, topk=topn),\n",
    "\"we-irbo p=0.9:\":word_embedding_irbo(topics,model.wv, weight=0.9, topk=topn)},index=[topic_type])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xQm6HntnRzr9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def make_graph_citydata(dataframe):\n",
    "    c_color = {}\n",
    "    for i in range(24):\n",
    "        c_color[i] = px.colors.qualitative.Light24[i]\n",
    "    edges = [(str(x),str(y)) for (x,y) in list(zip(dataframe.post_id.tolist(),dataframe.quote_id.tolist()))]\n",
    "    G = nx.MultiDiGraph()\n",
    "    for i in range(len(dataframe)):\n",
    "        G.add_node(dataframe.iloc[i]['post_id'],text=dataframe.iloc[i]['post'])\n",
    "                 \n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe.iloc[i]['quote_id'] != '' and dataframe.iloc[i]['quote_id'] not in G.nodes():\n",
    "            try:\n",
    "                G.add_node(dataframe.iloc[i]['quote_id'],text=dataframe.iloc[i]['quote'])\n",
    "            except:\n",
    "                G.add_node(dataframe.iloc[i]['quote_id'],text=None)\n",
    "        \n",
    "    G.add_edges_from(edges)\n",
    "    G.remove_node('')\n",
    "    \n",
    "    return G\n",
    "\n",
    "def clean_post(data):\n",
    "    posts = []\n",
    "    for post in data:\n",
    "        try:\n",
    "            s = soup(post)\n",
    "            for t in s.find_all('table'):\n",
    "                t.decompose()\n",
    "            div = s.find('div')\n",
    "            posts.append(re.subn('Quote:','',div.text)[0].strip())\n",
    "        except:\n",
    "            posts.append('')\n",
    "    return posts\n",
    "\n",
    "def get_paths_city_data(dataframe):\n",
    "    G = make_graph_citydata(dataframe)\n",
    "    sink_nodes = [node for node, outdegree in dict(G.out_degree(G.nodes())).items() if outdegree == 0]\n",
    "    source_nodes = [node for node, indegree in dict(G.in_degree(G.nodes())).items() if indegree == 0]\n",
    "    ss_nodes = [(source, sink) for sink in sink_nodes for source in source_nodes]\n",
    "    paths = []\n",
    "    for (source,sink) in ss_nodes:\n",
    "        for path in nx.all_simple_paths(G, source=source, target=sink):\n",
    "            paths.append(path)\n",
    "    return G, paths\n",
    "\n",
    "def make_thread_embeddings(dataframe, model):\n",
    "    id_text = {}\n",
    "    for i in range(len(dataframe)):\n",
    "        \n",
    "        id_text[dataframe.iloc[i]['quote_id']] = dataframe.iloc[i]['quote']\n",
    "        id_text[dataframe.iloc[i]['post_id']] = dataframe.iloc[i]['post']\n",
    "\n",
    "    G, paths = get_paths_city_data(dataframe)\n",
    "    chains = []\n",
    "    for path in paths:\n",
    "        p = []\n",
    "        for x in path:\n",
    "            try:\n",
    "                p.append(id_text[x])\n",
    "            except:\n",
    "                p.append('')\n",
    "        chains.append(p)\n",
    "    joint_chains = [' '.join(chain) for chain in chains]\n",
    "    embeddings = model.encode(joint_chains)\n",
    "    singletons = [node for node in G.nodes() if node not in flatten_list(paths)]\n",
    "    singleton_embeddings = model.encode([id_text[s] for s in singletons])\n",
    "    singleton_texts = [id_text[s] for s in singletons]\n",
    "    return G, paths, joint_chains, embeddings, singletons, singleton_embeddings,singleton_texts\n",
    "    \n",
    "def query_dataframe_quote(df,ie):\n",
    "    return [df.iloc[i]['cleaned_quote'] for i in range(len(df)) if df.iloc[i]['quote_id'] == ie][0]\n",
    "\n",
    "def query_dataframe_post(df,ie):\n",
    "    return [df.iloc[i]['post'] for i in range(len(df)) if df.iloc[i]['post_id'] == ie]\n",
    "\n",
    "def tfidf_merge(tuplelist):\n",
    "  d = {}\n",
    "  for (x,y) in tuplelist:\n",
    "    d[x] = []\n",
    "  for (x,y) in tuplelist:\n",
    "    d[x].append(y)\n",
    "  return list(zip(d.keys(), [sum(v) for v in d.values()]))\n",
    "\n",
    "import ast\n",
    "def reformat_quote_ids(ids):\n",
    "    qids = []\n",
    "    for i in ids:\n",
    "        try:\n",
    "            qids.append(int(ast.literal_eval(i)[0]))\n",
    "        except:\n",
    "            qids.append('')\n",
    "    return qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "def gensim_lda(texts, topic_num=5, topic_word_priors=None,numwords=25, eta_=None, tfidf=False):\n",
    "    \"\"\"gensim lda wrapper with guided topic modeling\"\"\"\n",
    "\n",
    "    if tfidf != False:\n",
    "        #process texts\n",
    "        #build gensim dictionary\n",
    "        processed_texts = [text_tokenize(text) for text in texts]\n",
    "        dictionary = gensim.corpora.Dictionary(processed_texts)\n",
    "    \n",
    "        #build bag-of-words representation\n",
    "        bow = [dictionary.doc2bow(text.split()) for text in texts]\n",
    "        tfidf = models.TfidfModel(bow)\n",
    "        corpus_tfidf = tfidf[bow]\n",
    "        #guided lda with eta\n",
    "        if topic_word_priors and eta_ != None:\n",
    "            etas = []\n",
    "        \n",
    "            for r in range(len(topic_word_priors)):\n",
    "                eta = []\n",
    "                for i in range(len(dictionary)):\n",
    "                    \n",
    "                    if dictionary[i] in topic_word_priors[r]:\n",
    "                        eta.append(np.array(eta_))\n",
    "                    else:\n",
    "                        eta.append(np.array(1/topic_num))\n",
    "                etas.append(eta)\n",
    "    \n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=corpus_tfidf, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=np.array(etas),\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "    \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[corpus_tfidf]\n",
    "    \n",
    "            #extract topical terms\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=numwords)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "            \n",
    "        else:\n",
    "            #standard lda\n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=corpus_tfidf, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=None,\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "            \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[corpus_tfidf]\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=numwords)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "    else:\n",
    "        #process texts\n",
    "        #build gensim dictionary\n",
    "        processed_texts = [text_tokenize(text) for text in texts]\n",
    "        dictionary = gensim.corpora.Dictionary(processed_texts)\n",
    "        \n",
    "        #build bag-of-words representation\n",
    "        bow = [dictionary.doc2bow(text.split()) for text in texts]\n",
    "        #tfidf = models.TfidfModel(bow)\n",
    "        #corpus_tfidf = tfidf[bow]\n",
    "    \n",
    "    \n",
    "        #guided lda with eta\n",
    "        if topic_word_priors and eta_ != None:\n",
    "            etas = []\n",
    "        \n",
    "            for r in range(len(topic_word_priors)):\n",
    "                eta = []\n",
    "                for i in range(len(dictionary)):\n",
    "                    \n",
    "                    if dictionary[i] in topic_word_priors[r]:\n",
    "                        eta.append(np.array(eta_))\n",
    "                    else:\n",
    "                        eta.append(np.array(1/topic_num))\n",
    "                etas.append(eta)\n",
    "    \n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=bow, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=np.array(etas),\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "    \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[bow]\n",
    "    \n",
    "            #extract topical terms\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=numwords)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "        \n",
    "        else:\n",
    "        #standard lda\n",
    "            model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=bow, id2word=dictionary, num_topics=topic_num,\n",
    "                random_state=42, chunksize=100, eta=None,\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "            \n",
    "            #transform corpus into topics\n",
    "            transformed_corpus = model[bow]\n",
    "            \n",
    "            #extract topical terms\n",
    "            topical_terms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=25)]\n",
    "            return model, topical_terms, transformed_corpus, processed_texts\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans, SpectralClustering, kmeans_plusplus\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "stops = stopwords.words('english') + ['said','know','maybe','post','advertisements','advertisement','posted','thread','like','could','should','would','thing']\n",
    "wn = WordNetLemmatizer()\n",
    "stemmer = nltk.PorterStemmer()\n",
    "def text_process(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if not re.findall(r'\\.com|___|Advertisements|-|_',token)]\n",
    "    \n",
    "    \n",
    "    return ' '.join([wn.lemmatize(token) for token in tokens if len(token) > 3 and token not in stops and not re.findall(r'[0-9]',token) and not re.findall(r'htt',token)])\n",
    "def text_tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    tokens = [token for token in tokens if not re.findall(r'\\.com|___|Advertisements|-|_',token)]\n",
    "    lemmas = [wn.lemmatize(token) for token in tokens if len(token) > 2 and token not in stops]\n",
    "    filtered = [lemma for lemma in lemmas if not re.findall(r'[0-9]',lemma) and not re.findall(r'htt',lemma)]\n",
    "    return filtered\n",
    "    \n",
    "    \n",
    "    \n",
    "# def cluster_thread_embeddings_spectral_tfidf(threads,embeddings, min_words=5, clusters=3,ngrams=(1,1)):\n",
    "#     sc = SpectralClustering(n_clusters=clusters)\n",
    "#     sc.fit(embeddings)\n",
    "\n",
    "\n",
    "#     processed_threads = [text_process(thread) for thread in threads]\n",
    "#     df = pd.DataFrame({'thread':processed_threads,'label':sc.labels_})\n",
    "#     Xtrain, xtest, ytrain, ytest = train_test_split(df.thread,df.label,stratify=df.label,test_size=.3)\n",
    "#     tfidf = TfidfVectorizer(stop_words='english',min_df=min_words,ngram_range=ngrams)\n",
    "#     tfidf.fit(Xtrain)\n",
    "\n",
    "#     Xtrain = tfidf.transform(Xtrain)\n",
    "#     xtest = tfidf.transform(xtest)\n",
    "#     lg = LogisticRegression(C=1,solver='newton-cg',max_iter=5000)\n",
    "\n",
    "#     kb = SelectKBest(chi2, k=20).fit(Xtrain, ytrain)\n",
    "#     X_ = kb.transform(Xtrain)\n",
    "#     x_test_ = kb.transform(xtest)\n",
    "#     lg.fit(X_,ytrain)\n",
    "#     predictions = lg.predict(x_test_)\n",
    "\n",
    "#     #metrics\n",
    "#     report_df = pd.DataFrame(classification_report(ytest,predictions,output_dict=True))\n",
    "#     #print(report_df)\n",
    "    \n",
    "#     # lr = LogisticRegression(C=1,solver='newton-cg',max_iter=5000)\n",
    "#     # lr.fit(tfidf.transform(df.thread),sc.labels_)\n",
    "#     ff = pd.DataFrame()\n",
    "#     ff['term'] =tfidf.get_feature_names_out()\n",
    "#     if clusters > 2:\n",
    "#         for i in range(clusters):\n",
    "#             ff['coef_'+str(i)] = tfidf.transform([' '.join(df.groupby('label')['thread'].apply(list)[i])]).toarray().tolist()[0]\n",
    "       \n",
    "#     else:\n",
    "#         ff['coef_0'] = tfidf.transform([' '.join(df.groupby('label')['thread'].apply(list)[i])]).toarray().tolist()[0]\n",
    "#         ff['coef_1'] = tfidf.transform([' '.join(df.groupby('label')['thread'].apply(list)[i])]).toarray().tolist()[0]\n",
    "#     ff = pd.DataFrame([ff.iloc[i] for i in range(len(ff)) if len(ff.iloc[i]['term']) > 3])\n",
    "\n",
    "#     topics = get_topical_terms(ff,term_count=25)\n",
    "#     print(get_topical_coherence(processed_threads, topics,metric='u_mass'))\n",
    "\n",
    "#     #plot(px.scatter(x=sevecs[:,0],y=sevecs[:,1],color=[str(l) for l in sc.labels_]))\n",
    "#     return df,ff,tfidf, sc, report_df, topics\n",
    "\n",
    "def cluster_thread_embeddings_kmeans_tfidf(threads,embeddings, clusters=3,vectorizer='cv',mindf=5, ngrams=(1,1),kbest=5000,init_='k-means++'):\n",
    "    #if cluster centers provided:\n",
    "    if type(init_) == np.ndarray:\n",
    "        print('setting cluster centers')\n",
    "     \n",
    "    km = KMeans(n_clusters=clusters,init=init_, random_state=0)\n",
    "    km.fit(embeddings)\n",
    "   \n",
    "\n",
    "    processed_threads = [text_process(thread) for thread in threads]\n",
    "    df = pd.DataFrame({'thread':processed_threads,'label':km.labels_})\n",
    "\n",
    "    df_grouped = df.groupby('label')['thread'].apply(list)\n",
    "\n",
    "    if vectorizer == 'cv':\n",
    "        cv = CountVectorizer(stop_words='english',min_df=mindf,ngram_range=ngrams)\n",
    "        #ctfidf = TfidfVectorizer(stop_words='english',min_df=mindf, ngram_range=ngrams)\n",
    "        CX = cv.fit_transform(df.thread)\n",
    "        kbc = SelectKBest(chi2, k=kbest).fit(CX, km.labels_)\n",
    "\n",
    "        ff = pd.DataFrame()\n",
    "        ff['term'] = np.asarray(cv.get_feature_names_out())[kbc.get_support()]\n",
    "        if clusters > 2:\n",
    "            for i in range(clusters):\n",
    "                ff['coef_'+str(i)] = kbc.transform(cv.transform([' '.join(df.groupby('label')['thread'].apply(list)[i])])).toarray().tolist()[0]\n",
    "                                                 \n",
    "        else:\n",
    "            ff['coef_0'] = kbc.transform(cv.transform([' '.join(df.groupby('label')['thread'].apply(list)[i])])).toarray().tolist()[0]\n",
    "            ff['coef_1'] = kbc.transform(cv.transform([' '.join(df.groupby('label')['thread'].apply(list)[i])])).toarray().tolist()[0]\n",
    "            \n",
    "        ff = pd.DataFrame([ff.iloc[i] for i in range(len(ff)) if len(ff.iloc[i]['term']) > 3])\n",
    "        topics = get_topical_terms(ff,term_count=25)\n",
    "        print(get_topical_coherence(processed_threads, topics,metric='u_mass'))\n",
    "        return {'threads':df,'term_weights':ff,'vectorizer':cv, 'clusterer':km, 'topics':topics}\n",
    "   \n",
    "    elif vectorizer == 'tfidf':\n",
    "        tfidf = TfidfVectorizer(stop_words='english',min_df=mindf, ngram_range=ngrams)\n",
    "        CX = tfidf.fit_transform(df.thread)\n",
    "        kbc = SelectKBest(chi2, k=kbest).fit(CX, km.labels_)\n",
    "\n",
    "        ff = pd.DataFrame()\n",
    "        ff['term'] = np.asarray(tfidf.get_feature_names_out())[kbc.get_support()]\n",
    "        if clusters > 2:\n",
    "            for i in range(clusters):\n",
    "                ff['coef_'+str(i)] = kbc.transform(tfidf.transform([' '.join(df.groupby('label')['thread'].apply(list)[i])])).toarray().tolist()[0]\n",
    "                                                 \n",
    "        else:\n",
    "            ff['coef_0'] = kbc.transform(tfidf.transform([' '.join(df.groupby('label')['thread'].apply(list)[i])])).toarray().tolist()[0]\n",
    "            ff['coef_1'] = kbc.transform(tfidf.transform([' '.join(df.groupby('label')['thread'].apply(list)[i])])).toarray().tolist()[0]\n",
    "            \n",
    "        ff = pd.DataFrame([ff.iloc[i] for i in range(len(ff)) if len(ff.iloc[i]['term']) > 3])\n",
    "        topics = get_topical_terms(ff,term_count=25)\n",
    "        print(get_topical_coherence(processed_threads, topics,metric='u_mass'))\n",
    "\n",
    "        return {'threads':df,'term_weights':ff,'vectorizer':tfidf, 'clusterer':km, 'topics':topics}\n",
    "        \n",
    "   \n",
    "\n",
    "# def cluster_thread_embeddings_spectral(threads,embeddings, min_words=5, clusters=3,ngrams=(0,1)):\n",
    "#     sc = SpectralClustering(n_clusters=clusters)\n",
    "#     sc.fit(embeddings)\n",
    "\n",
    "\n",
    "#     processed_threads = [text_process(thread) for thread in threads]\n",
    "#     df = pd.DataFrame({'thread':processed_threads,'label':sc.labels_})\n",
    "#     Xtrain, xtest, ytrain, ytest = train_test_split(df.thread,df.label,test_size=.3)\n",
    "#     tfidf = TfidfVectorizer(stop_words='english',min_df=min_words,ngram_range=ngrams)\n",
    "#     tfidf.fit(Xtrain)\n",
    "\n",
    "#     Xtrain = tfidf.transform(Xtrain)\n",
    "#     xtest = tfidf.transform(xtest)\n",
    "#     lg = LogisticRegression(C=1,solver='newton-cg',max_iter=5000)\n",
    "#     lg.fit(Xtrain,ytrain)\n",
    "#     predictions = lg.predict(xtest)\n",
    "\n",
    "#     #metrics\n",
    "#     report_df = pd.DataFrame(classification_report(ytest,predictions,output_dict=True))\n",
    "#     #print(report_df)\n",
    "    \n",
    "#     lr = LogisticRegression(C=1,solver='newton-cg',max_iter=5000)\n",
    "#     lr.fit(tfidf.transform(df.thread),sc.labels_)\n",
    "#     ff = pd.DataFrame()\n",
    "#     ff['term'] =tfidf.get_feature_names_out()\n",
    "#     if clusters > 2:\n",
    "#         for i in range(clusters):\n",
    "#             ff['coef_'+str(i)] = lr.coef_[i]\n",
    "       \n",
    "#     else:\n",
    "#         ff['coef_0'] = lr.coef_[0]\n",
    "#         ff['coef_1'] = lr.coef_[0] * -1\n",
    "#     ff = pd.DataFrame([ff.iloc[i] for i in range(len(ff)) if len(ff.iloc[i]['term']) > 3])\n",
    "\n",
    "    \n",
    "#     se = SpectralEmbedding(n_components=2)\n",
    "#     sevecs = se.fit_transform(embeddings)\n",
    "\n",
    "#     #plot(px.scatter(x=sevecs[:,0],y=sevecs[:,1],color=[str(l) for l in sc.labels_]))\n",
    "#     return df,ff,tfidf, lr, sc, report_df, sevecs\n",
    "    \n",
    "# def cluster_thread_embeddings_kmeans(threads,embeddings,clusters=3,ngrams=(1,1)):\n",
    "#     km = KMeans(n_clusters=clusters)\n",
    "#     km.fit(embeddings)\n",
    "\n",
    "\n",
    "#     processed_threads = [text_process(thread) for thread in threads]\n",
    "#     df = pd.DataFrame({'thread':processed_threads,'label':km.labels_})\n",
    "    \n",
    "#     Xtrain, xtest, ytrain, ytest = train_test_split(df.thread.tolist(),df.label.tolist(),test_size=.3)\n",
    "#     tfidf = TfidfVectorizer(stop_words='english',min_df=5,ngram_range=ngrams)\n",
    "#     tfidf.fit(Xtrain)\n",
    "    \n",
    "#     Xtrain = tfidf.transform(Xtrain)\n",
    "#     xtest= tfidf.transform(xtest)\n",
    "\n",
    "#     lg = LogisticRegression(max_iter=5000)\n",
    "#     lg.fit(Xtrain,ytrain)\n",
    "#     predictions = lg.predict(xtest)\n",
    "   \n",
    "#     report_df = pd.DataFrame(classification_report(ytest,predictions,output_dict=True))\n",
    "#     #print(report_df)\n",
    "    \n",
    "\n",
    "#     lr = LogisticRegression(C=.5,max_iter=5000)\n",
    "#     lr.fit(tfidf.transform(df.thread.tolist()),df.label.tolist())\n",
    "#     ff = pd.DataFrame()\n",
    "#     ff['term'] =tfidf.get_feature_names_out()\n",
    "#     if clusters > 2:\n",
    "#         for i in range(clusters):\n",
    "#             ff['coef_'+str(i)] = lr.coef_[i]\n",
    "       \n",
    "        \n",
    "#     else:\n",
    "#         ff['coef_0'] = lr.coef_[0]\n",
    "#         ff['coef_1'] = lr.coef_[0] * -1\n",
    "        \n",
    "#     ff = pd.DataFrame([ff.iloc[i] for i in range(len(ff)) if len(ff.iloc[i]['term']) > 3])\n",
    "#     se = SpectralEmbedding(n_components=2)\n",
    "#     sevecs = se.fit_transform(embeddings)\n",
    "\n",
    "#     #plot(px.scatter(x=sevecs[:,0],y=sevecs[:,1],color=[str(l) for l in km.labels_]))\n",
    "#     return df,ff,tfidf, lr, km, report_df, sevecs\n",
    "# def cluster_thread_embeddings_hdbscan(threads,embeddings,min_cluster_size=5,ngrams=(1,1),metric='manhattan'):\n",
    "#     hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, gen_min_span_tree=True,metric=metric,allow_single_cluster=False)\n",
    "#     hdb.fit(embeddings)\n",
    "\n",
    "\n",
    "#     processed_threads = [text_process(thread) for thread in threads]\n",
    "#     df = pd.DataFrame({'thread':processed_threads,'label':hdb.labels_})\n",
    "    \n",
    "#     Xtrain, xtest, ytrain, ytest = train_test_split(df.thread.tolist(),df.label.tolist(),test_size=.2)\n",
    "#     tfidf = TfidfVectorizer(stop_words='english',min_df=5,ngram_range=ngrams)\n",
    "#     tfidf.fit(Xtrain)\n",
    "    \n",
    "#     Xtrain = tfidf.transform(Xtrain)\n",
    "#     xtest= tfidf.transform(xtest)\n",
    "\n",
    "#     lg = LogisticRegression(max_iter=5000)\n",
    "#     lg.fit(Xtrain,ytrain)\n",
    "#     predictions = lg.predict(xtest)\n",
    "   \n",
    "#     report_df = pd.DataFrame(classification_report(ytest,predictions,output_dict=True))\n",
    "#     #print(report_df)\n",
    "    \n",
    "\n",
    "#     lr = LogisticRegression(C=.001,max_iter=5000)\n",
    "#     lr.fit(tfidf.transform(df.thread.tolist()),df.label.tolist())\n",
    "#     ff = pd.DataFrame()\n",
    "#     ff['term'] =tfidf.get_feature_names_out()\n",
    "\n",
    "#     clusters = len(list(set(hdb.labels_)))\n",
    "#     if clusters > 2:\n",
    "#         for i in range(clusters):\n",
    "#             ff['coef_'+str(i)] = lr.coef_[i]\n",
    "       \n",
    "#     else:\n",
    "#         ff['coef_0'] = lr.coef_[0]\n",
    "#         ff['coef_1'] = lr.coef_[0] * -1\n",
    "\n",
    "#     ff = pd.DataFrame([ff.iloc[i] for i in range(len(ff)) if len(ff.iloc[i]['term']) > 3])\n",
    "    \n",
    "#     se = SpectralEmbedding(n_components=2)\n",
    "#     sevecs = se.fit_transform(embeddings)\n",
    "#     return df,ff,tfidf, lr, hdb, report_df, sevecs\n",
    "\n",
    "# def cluster_thread_embeddings(threads, embeddings,cluster_type='spectral', clusters=3):\n",
    "#     if cluster_type == 'spectral':\n",
    "#         df, ff, tfidf, lr, sc = cluster_thread_embeddings_spectral(threads, embeddings, clusters)\n",
    "#         return df, ff, tfidf, lr, sc\n",
    "#     elif cluster_type == 'kmeans':\n",
    "#         df, ff, tfidf, lr, km = cluster_thread_embeddings_kmeans(threads, embeddings, clusters)\n",
    "#         return df, ff, tfidf, lr, km\n",
    "\n",
    "def fcluster_thread_embeddings(threads,embeddings,threshold=2):\n",
    "    \n",
    "    processed_threads = [text_process(thread) for thread in threads]\n",
    "\n",
    "    \n",
    "    ls = fclusterdata(embeddings,t=threshold,criterion='distance',method='ward')\n",
    "        \n",
    "    df = pd.DataFrame({'thread':processed_threads,'label':ls})\n",
    "    Xtrain, xtest, ytrain, ytest = train_test_split(df.thread,df.label,test_size=.2)\n",
    "    tfidf = TfidfVectorizer(stop_words='english',min_df=5,ngram_range=(1,3))\n",
    "    \n",
    "    lg = LogisticRegression()\n",
    "    lg.fit(tfidf.fit_transform(Xtrain),ytrain)\n",
    "    predictions = lg.predict(tfidf.transform(xtest))\n",
    "    print(classification_report(ytest,predictions))\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(tfidf.transform(df.thread),ls)\n",
    "    ff = pd.DataFrame()\n",
    "    ff['term'] =tfidf.get_feature_names_out()\n",
    "    for i in range(len(Counter(ls))):\n",
    "        ff['coef_'+str(i)] = lr.coef_[i]\n",
    "       \n",
    "    ff = pd.DataFrame([ff.iloc[i] for i in range(len(ff)) if len(ff.iloc[i]['term']) > 3])\n",
    "    \n",
    "    # se = SpectralEmbedding(n_components=2)\n",
    "    # sevecs = se.fit_transform(embeddings)\n",
    "    \n",
    "    #plot(px.scatter(x=sevecs[:,0],y=sevecs[:,1],color=[str(l) for l in ls]))\n",
    "\n",
    "    return df, ff\n",
    "\n",
    "#corpus_file = api.load('crawl-300d-2M-subword/crawl-300d-2M-subword.bin')\n",
    "\n",
    "# with open('crawl-300d-2M-subword/crawl-300d-2M-subword.bin', 'rb') as fin:\n",
    "#     fast = gensim.models.fasttext.load_facebook_model(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast = gensim.models.fasttext.load_facebook_model('crawl-300d-2M-subword/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "with open('crawl-300d-2M-subword/crawl-300d-2M-subword.bin', 'rb') as fin:\n",
    "    fast = gensim.models.fasttext.load_facebook_model(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "from __future__ imports must occur at the beginning of the file (2635683237.py, line 186)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\rmomi\\AppData\\Local\\Temp\\ipykernel_24980\\2635683237.py\"\u001b[1;36m, line \u001b[1;32m186\u001b[0m\n\u001b[1;33m    from bisect import bisect_left\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def proportion_unique_words(topics, topk=10):\n",
    "    \"\"\"\n",
    "    compute the proportion of unique words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    topk: top k words on which the topic diversity will be computed\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than '+str(topk))\n",
    "    else:\n",
    "        unique_words = set()\n",
    "        for topic in topics:\n",
    "            unique_words = unique_words.union(set(topic[:topk]))\n",
    "        puw = len(unique_words) / (topk * len(topics))\n",
    "        return puw\n",
    "\n",
    "\n",
    "def irbo(topics, weight=0.9, topk=10):\n",
    "    \"\"\"\n",
    "    compute the inverted rank-biased overlap\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    weight: p (float), default 1.0: Weight of each\n",
    "        agreement at depth d:p**(d-1). When set\n",
    "        to 1.0, there is no weight, the rbo returns\n",
    "        to average overlap.\n",
    "    topk: top k words on which the topic diversity\n",
    "          will be computed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    irbo : score of the rank biased overlap over the topics\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than topk')\n",
    "    else:\n",
    "        collect = []\n",
    "        for list1, list2 in combinations(topics, 2):\n",
    "            word2index = get_word2index(list1, list2)\n",
    "            indexed_list1 = [word2index[word] for word in list1]\n",
    "            indexed_list2 = [word2index[word] for word in list2]\n",
    "            rbo_val = rbo(indexed_list1[:topk], indexed_list2[:topk], p=weight)[2]\n",
    "            collect.append(rbo_val)\n",
    "        return 1 - np.mean(collect)\n",
    "\n",
    "\n",
    "def word_embedding_irbo(topics, word_embedding_model, weight=0.9, topk=10):\n",
    "    '''\n",
    "    compute the word embedding-based inverted rank-biased overlap\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    weight: p (float), default 1.0: Weight of each agreement at depth d:\n",
    "    p**(d-1). When set to 1.0, there is no weight, the rbo returns to average overlap.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    weirbo: word embedding-based inverted rank_biased_overlap over the topics\n",
    "    '''\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than topk')\n",
    "    else:\n",
    "        collect = []\n",
    "        for list1, list2 in combinations(topics, 2):\n",
    "            word2index = get_word2index(list1, list2)\n",
    "            index2word = {v: k for k, v in word2index.items()}\n",
    "            indexed_list1 = [word2index[word] for word in list1]\n",
    "            indexed_list2 = [word2index[word] for word in list2]\n",
    "            rbo_val = word_embeddings_rbo(indexed_list1[:topk], indexed_list2[:topk], p=weight,\n",
    "                                          index2word=index2word, word2vec=word_embedding_model)[2]\n",
    "            collect.append(rbo_val)\n",
    "        return 1 - np.mean(collect)\n",
    "\n",
    "\n",
    "def pairwise_jaccard_diversity(topics, topk=10):\n",
    "    '''\n",
    "    compute the average pairwise jaccard distance between the topics \n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    topk: top k words on which the topic diversity\n",
    "          will be computed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pjd: average pairwise jaccard distance\n",
    "    '''\n",
    "    dist = 0\n",
    "    count = 0\n",
    "    for list1, list2 in combinations(topics, 2):\n",
    "        js = 1 - len(set(list1).intersection(set(list2)))/len(set(list1).union(set(list2)))\n",
    "        dist = dist + js\n",
    "        count = count + 1\n",
    "    return dist/count\n",
    "\n",
    "\n",
    "def pairwise_word_embedding_distance(topics, word_embedding_model, topk=10):\n",
    "    \"\"\"\n",
    "    :param topk: how many most likely words to consider in the evaluation\n",
    "    :return: topic coherence computed on the word embeddings similarities\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than topk')\n",
    "    else:\n",
    "        count = 0\n",
    "        sum_dist = 0\n",
    "        for list1, list2 in combinations(topics, 2):\n",
    "            count = count+1\n",
    "            word_counts = 0\n",
    "            dist = 0\n",
    "            for word1 in list1[:topk]:\n",
    "                for word2 in list2[:topk]:\n",
    "                    dist = dist + distance.cosine(word_embedding_model.wv[word1], word_embedding_model.wv[word2])\n",
    "                    word_counts = word_counts + 1\n",
    "\n",
    "            dist = dist/word_counts\n",
    "            sum_dist = sum_dist + dist\n",
    "        return sum_dist/count\n",
    "\n",
    "\n",
    "def centroid_distance(topics, word_embedding_model, topk=10):\n",
    "    \"\"\"\n",
    "    :param topk: how many most likely words to consider in the evaluation\n",
    "    :return: topic coherence computed on the word embeddings similarities\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than topk')\n",
    "    else:\n",
    "        count = 0\n",
    "        for list1, list2 in combinations(topics, 2):\n",
    "            count = count + 1\n",
    "            centroid1 = np.zeros(word_embedding_model.vector_size)\n",
    "            centroid2 = np.zeros(word_embedding_model.vector_size)\n",
    "            for word1 in list1[:topk]:\n",
    "                centroid1 = centroid1 + word_embedding_model[word1]\n",
    "            for word2 in list2[:topk]:\n",
    "                centroid2 = centroid2 + word_embedding_model[word2]\n",
    "            centroid1 = centroid1 / len(list1[:topk])\n",
    "            centroid2 = centroid2 / len(list2[:topk])\n",
    "        return distance.cosine(centroid1, centroid2)\n",
    "\n",
    "\n",
    "def get_word2index(list1, list2):\n",
    "    words = set(list1)\n",
    "    words = words.union(set(list2))\n",
    "    word2index = {w: i for i, w in enumerate(words)}\n",
    "    return word2index\n",
    "\"\"\"Rank-biased overlap, a ragged sorted list similarity measure.\n",
    "See http://doi.acm.org/10.1145/1852102.1852106 for details. All functions\n",
    "directly corresponding to concepts from the paper are named so that they can be\n",
    "clearly cross-identified.\n",
    "The definition of overlap has been modified to account for ties. Without this,\n",
    "results for lists with tied items were being inflated. The modification itself\n",
    "is not mentioned in the paper but seems to be reasonable, see function\n",
    "``overlap()``. Places in the code which diverge from the spec in the paper\n",
    "because of this are highlighted with comments.\n",
    "The two main functions for performing an RBO analysis are ``rbo()`` and\n",
    "``rbo_dict()``; see their respective docstrings for how to use them.\n",
    "The following doctest just checks that equivalent specifications of a\n",
    "problem yield the same result using both functions:\n",
    "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
    "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
    "    >>> ans_rbo = _round(rbo(lst1, lst2, p=.9))\n",
    "    >>> dct1 = dict(a=1, b=2, c=1, d=3)\n",
    "    >>> dct2 = dict(a=1, b=2, c=2, d=3)\n",
    "    >>> ans_rbo_dict = _round(rbo_dict(dct1, dct2, p=.9, sort_ascending=True))\n",
    "    >>> ans_rbo == ans_rbo_dict\n",
    "    True\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "from bisect import bisect_left\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "RBO = namedtuple(\"RBO\", \"min res ext\")\n",
    "RBO.__doc__ += \": Result of full RBO analysis\"\n",
    "RBO.min.__doc__ = \"Lower bound estimate\"\n",
    "RBO.res.__doc__ = \"Residual corresponding to min; min + res is an upper bound estimate\"\n",
    "RBO.ext.__doc__ = \"Extrapolated point estimate\"\n",
    "\n",
    "\n",
    "def _round(obj):\n",
    "    if isinstance(obj, RBO):\n",
    "        return RBO(_round(obj.min), _round(obj.res), _round(obj.ext))\n",
    "    else:\n",
    "        return round(obj, 3)\n",
    "\n",
    "\n",
    "def set_at_depth(lst, depth):\n",
    "    ans = set()\n",
    "    for v in lst[:depth]:\n",
    "        if isinstance(v, set):\n",
    "            ans.update(v)\n",
    "        else:\n",
    "            ans.add(v)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def raw_overlap(list1, list2, depth):\n",
    "    \"\"\"Overlap as defined in the article.\n",
    "    \"\"\"\n",
    "    set1, set2 = set_at_depth(list1, depth), set_at_depth(list2, depth)\n",
    "    return len(set1.intersection(set2)), len(set1), len(set2)\n",
    "\n",
    "\n",
    "def overlap(list1, list2, depth):\n",
    "    \"\"\"Overlap which accounts for possible ties.\n",
    "    This isn't mentioned in the paper but should be used in the ``rbo*()``\n",
    "    functions below, otherwise overlap at a given depth might be > depth which\n",
    "    inflates the result.\n",
    "    There are no guidelines in the paper as to what's a good way to calculate\n",
    "    this, but a good guess is agreement scaled by the minimum between the\n",
    "    requested depth and the lengths of the considered lists (overlap shouldn't\n",
    "    be larger than the number of ranks in the shorter list, otherwise results\n",
    "    are conspicuously wrong when the lists are of unequal lengths -- rbo_ext is\n",
    "    not between rbo_min and rbo_min + rbo_res.\n",
    "    >>> overlap(\"abcd\", \"abcd\", 3)\n",
    "    3.0\n",
    "    >>> overlap(\"abcd\", \"abcd\", 5)\n",
    "    4.0\n",
    "    >>> overlap([\"a\", {\"b\", \"c\"}, \"d\"], [\"a\", {\"b\", \"c\"}, \"d\"], 2)\n",
    "    2.0\n",
    "    >>> overlap([\"a\", {\"b\", \"c\"}, \"d\"], [\"a\", {\"b\", \"c\"}, \"d\"], 3)\n",
    "    3.0\n",
    "    \"\"\"\n",
    "    ov = agreement(list1, list2, depth) * min(depth, len(list1), len(list2))\n",
    "    return ov\n",
    "    # NOTE: comment the preceding and uncomment the following line if you want\n",
    "    # to stick to the algorithm as defined by the paper\n",
    "    # return raw_overlap(list1, list2, depth)[0]\n",
    "\n",
    "\n",
    "def agreement(list1, list2, depth):\n",
    "    \"\"\"Proportion of shared values between two sorted lists at given depth.\n",
    "    >>> _round(agreement(\"abcde\", \"abdcf\", 1))\n",
    "    1.0\n",
    "    >>> _round(agreement(\"abcde\", \"abdcf\", 3))\n",
    "    0.667\n",
    "    >>> _round(agreement(\"abcde\", \"abdcf\", 4))\n",
    "    1.0\n",
    "    >>> _round(agreement(\"abcde\", \"abdcf\", 5))\n",
    "    0.8\n",
    "    >>> _round(agreement([{1, 2}, 3], [1, {2, 3}], 1))\n",
    "    0.667\n",
    "    >>> _round(agreement([{1, 2}, 3], [1, {2, 3}], 2))\n",
    "    1.0\n",
    "    \"\"\"\n",
    "    len_intersection, len_set1, len_set2 = raw_overlap(list1, list2, depth)\n",
    "    return 2 * len_intersection / (len_set1 + len_set2)\n",
    "\n",
    "\n",
    "def cumulative_agreement(list1, list2, depth):\n",
    "    return (agreement(list1, list2, d) for d in range(1, depth + 1))\n",
    "\n",
    "\n",
    "def average_overlap(list1, list2, depth=None):\n",
    "    \"\"\"Calculate average overlap between ``list1`` and ``list2``.\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 1))\n",
    "    0.0\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 2))\n",
    "    0.0\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 3))\n",
    "    0.222\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 4))\n",
    "    0.292\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 5))\n",
    "    0.313\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 6))\n",
    "    0.317\n",
    "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 7))\n",
    "    0.312\n",
    "    \"\"\"\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    return sum(cumulative_agreement(list1, list2, depth)) / depth\n",
    "\n",
    "\n",
    "def rbo_at_k(list1, list2, p, depth=None):\n",
    "    # ``p**d`` here instead of ``p**(d - 1)`` because enumerate starts at\n",
    "    # 0\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    d_a = enumerate(cumulative_agreement(list1, list2, depth))\n",
    "    return (1 - p) * sum(p ** d * a for (d, a) in d_a)\n",
    "\n",
    "\n",
    "def rbo_min(list1, list2, p, depth=None):\n",
    "    \"\"\"Tight lower bound on RBO.\n",
    "    See equation (11) in paper.\n",
    "    >>> _round(rbo_min(\"abcdefg\", \"abcdefg\", .9))\n",
    "    0.767\n",
    "    >>> _round(rbo_min(\"abcdefgh\", \"abcdefg\", .9))\n",
    "    0.767\n",
    "    \"\"\"\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    x_k = overlap(list1, list2, depth)\n",
    "    log_term = x_k * math.log(1 - p)\n",
    "    sum_term = sum(\n",
    "        p ** d / d * (overlap(list1, list2, d) - x_k) for d in range(1, depth + 1)\n",
    "    )\n",
    "    return (1 - p) / p * (sum_term - log_term)\n",
    "\n",
    "\n",
    "def rbo_res(list1, list2, p):\n",
    "    \"\"\"Upper bound on residual overlap beyond evaluated depth.\n",
    "    See equation (30) in paper.\n",
    "    NOTE: The doctests weren't verified against manual computations but seem\n",
    "    plausible. In particular, for identical lists, ``rbo_min()`` and\n",
    "    ``rbo_res()`` should add up to 1, which is the case.\n",
    "    >>> _round(rbo_res(\"abcdefg\", \"abcdefg\", .9))\n",
    "    0.233\n",
    "    >>> _round(rbo_res(\"abcdefg\", \"abcdefghijklmnopqrstuvwxyz\", .9))\n",
    "    0.239\n",
    "    \"\"\"\n",
    "    S, L = sorted((list1, list2), key=len)\n",
    "    s, l = len(S), len(L)\n",
    "    x_l = overlap(list1, list2, l)\n",
    "    # since overlap(...) can be fractional in the general case of ties and f\n",
    "    # must be an integer --> math.ceil()\n",
    "    f = int(math.ceil(l + s - x_l))\n",
    "    # upper bound of range() is non-inclusive, therefore + 1 is needed\n",
    "    term1 = s * sum(p ** d / d for d in range(s + 1, f + 1))\n",
    "    term2 = l * sum(p ** d / d for d in range(l + 1, f + 1))\n",
    "    term3 = x_l * (math.log(1 / (1 - p)) - sum(p ** d / d for d in range(1, f + 1)))\n",
    "    return p ** s + p ** l - p ** f - (1 - p) / p * (term1 + term2 + term3)\n",
    "\n",
    "\n",
    "def rbo_ext(list1, list2, p):\n",
    "    \"\"\"RBO point estimate based on extrapolating observed overlap.\n",
    "    See equation (32) in paper.\n",
    "    NOTE: The doctests weren't verified against manual computations but seem\n",
    "    plausible.\n",
    "    >>> _round(rbo_ext(\"abcdefg\", \"abcdefg\", .9))\n",
    "    1.0\n",
    "    >>> _round(rbo_ext(\"abcdefg\", \"bacdefg\", .9))\n",
    "    0.9\n",
    "    \"\"\"\n",
    "    S, L = sorted((list1, list2), key=len)\n",
    "    s, l = len(S), len(L)\n",
    "    x_l = overlap(list1, list2, l)\n",
    "    x_s = overlap(list1, list2, s)\n",
    "    # the paper says overlap(..., d) / d, but it should be replaced by\n",
    "    # agreement(..., d) defined as per equation (28) so that ties are handled\n",
    "    # properly (otherwise values > 1 will be returned)\n",
    "    # sum1 = sum(p**d * overlap(list1, list2, d)[0] / d for d in range(1, l + 1))\n",
    "    sum1 = sum(p ** d * agreement(list1, list2, d) for d in range(1, l + 1))\n",
    "    sum2 = sum(p ** d * x_s * (d - s) / s / d for d in range(s + 1, l + 1))\n",
    "    term1 = (1 - p) / p * (sum1 + sum2)\n",
    "    term2 = p ** l * ((x_l - x_s) / l + x_s / s)\n",
    "    return term1 + term2\n",
    "\n",
    "\n",
    "def rbo(list1, list2, p):\n",
    "    \"\"\"Complete RBO analysis (lower bound, residual, point estimate).\n",
    "    ``list`` arguments should be already correctly sorted iterables and each\n",
    "    item should either be an atomic value or a set of values tied for that\n",
    "    rank. ``p`` is the probability of looking for overlap at rank k + 1 after\n",
    "    having examined rank k.\n",
    "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
    "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
    "    >>> _round(rbo(lst1, lst2, p=.9))\n",
    "    RBO(min=0.489, res=0.477, ext=0.967)\n",
    "    \"\"\"\n",
    "    if not 0 <= p <= 1:\n",
    "        raise ValueError(\"The ``p`` parameter must be between 0 and 1.\")\n",
    "    args = (list1, list2, p)\n",
    "    return RBO(rbo_min(*args), rbo_res(*args), rbo_ext(*args))\n",
    "\n",
    "\n",
    "def sort_dict(dct, *, ascending=False):\n",
    "    \"\"\"Sort keys in ``dct`` according to their corresponding values.\n",
    "    Sorts in descending order by default, because the values are\n",
    "    typically scores, i.e. the higher the better. Specify\n",
    "    ``ascending=True`` if the values are ranks, or some sort of score\n",
    "    where lower values are better.\n",
    "    Ties are handled by creating sets of tied keys at the given position\n",
    "    in the sorted list.\n",
    "    >>> dct = dict(a=1, b=2, c=1, d=3)\n",
    "    >>> list(sort_dict(dct)) == ['d', 'b', {'a', 'c'}]\n",
    "    True\n",
    "    >>> list(sort_dict(dct, ascending=True)) == [{'a', 'c'}, 'b', 'd']\n",
    "    True\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    items = []\n",
    "    # items should be unique, scores don't have to\n",
    "    for item, score in dct.items():\n",
    "        if not ascending:\n",
    "            score *= -1\n",
    "        i = bisect_left(scores, score)\n",
    "        if i == len(scores):\n",
    "            scores.append(score)\n",
    "            items.append(item)\n",
    "        elif scores[i] == score:\n",
    "            existing_item = items[i]\n",
    "            if isinstance(existing_item, set):\n",
    "                existing_item.add(item)\n",
    "            else:\n",
    "                items[i] = {existing_item, item}\n",
    "        else:\n",
    "            scores.insert(i, score)\n",
    "            items.insert(i, item)\n",
    "    return items\n",
    "\n",
    "\n",
    "def rbo_dict(dict1, dict2, p, *, sort_ascending=False):\n",
    "    \"\"\"Wrapper around ``rbo()`` for dict input.\n",
    "    Each dict maps items to be sorted to the score according to which\n",
    "    they should be sorted. The RBO analysis is then performed on the\n",
    "    resulting sorted lists.\n",
    "    The sort is descending by default, because scores are typically the\n",
    "    higher the better, but this can be overridden by specifying\n",
    "    ``sort_ascending=True``.\n",
    "    >>> dct1 = dict(a=1, b=2, c=1, d=3)\n",
    "    >>> dct2 = dict(a=1, b=2, c=2, d=3)\n",
    "    >>> _round(rbo_dict(dct1, dct2, p=.9, sort_ascending=True))\n",
    "    RBO(min=0.489, res=0.477, ext=0.967)\n",
    "    \"\"\"\n",
    "    list1, list2 = (\n",
    "        sort_dict(dict1, ascending=sort_ascending),\n",
    "        sort_dict(dict2, ascending=sort_ascending),\n",
    "    )\n",
    "    return rbo(list1, list2, p)\n",
    "\n",
    "\n",
    "if __name__ in (\"__main__\", \"__console__\"):\n",
    "    import doctest\n",
    "\n",
    "    doctest.testmod()\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import math\n",
    "from bisect import bisect_left\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "RBO = namedtuple(\"RBO\", \"min res ext\")\n",
    "RBO.__doc__ += \": Result of full RBO analysis\"\n",
    "RBO.min.__doc__ = \"Lower bound estimate\"\n",
    "RBO.res.__doc__ = \"Residual corresponding to min; min + res is an upper bound estimate\"\n",
    "RBO.ext.__doc__ = \"Extrapolated point estimate\"\n",
    "\n",
    "def _round(obj):\n",
    "    if isinstance(obj, RBO):\n",
    "        return RBO(_round(obj.min), _round(obj.res), _round(obj.ext))\n",
    "    else:\n",
    "        return round(obj, 3)\n",
    "\n",
    "\n",
    "def set_at_depth(lst, depth):\n",
    "    ans = set()\n",
    "    for v in lst[:depth]:\n",
    "        if isinstance(v, set):\n",
    "            ans.update(v)\n",
    "        else:\n",
    "            ans.add(v)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def embeddings_overlap(list1, list2, depth, index2word, word2vec):\n",
    "    #set1, set2 = set_at_depth(list1, depth), set_at_depth(list2, depth)\n",
    "    #return len(set1.intersection(set2)), len(set1), len(set2)\n",
    "\n",
    "    set1, set2 = set_at_depth(list1, depth), set_at_depth(list2, depth)\n",
    "    word_list1 = [index2word[index] for index in list1]\n",
    "    word_list2 = [index2word[index] for index in list2]\n",
    "\n",
    "    similarities = {}\n",
    "    for w1 in word_list1[:depth]:\n",
    "        for w2 in word_list2[:depth]:\n",
    "            similarities[(w1,w2)] = word2vec.similarity(w1, w2)\n",
    "\n",
    "    similarities = OrderedDict(sorted(similarities.items(), key=lambda x: -x[1]))\n",
    "\n",
    "    e_ov = 0\n",
    "    key_list = list(similarities.keys())\n",
    "    for k in key_list:\n",
    "        if k in similarities.keys():\n",
    "            #print(k, similarities[k])\n",
    "            e_ov = e_ov + similarities[k]\n",
    "            similarities = {save_k: v for save_k, v in similarities.items()\n",
    "                            if save_k[0] != k[0] and save_k[1] != k[1]}\n",
    "    #e_ov = 1\n",
    "    #print(\"****\")\n",
    "    return e_ov, len(set1), len(set2)\n",
    "\n",
    "\n",
    "def overlap(list1, list2, depth, index2word, word2vec):\n",
    "    #return agreement(list1, list2, depth) * min(depth, len(list1), len(list2))\n",
    "    # NOTE: comment the preceding and uncomment the following line if you want\n",
    "    # to stick to the algorithm as defined by the paper\n",
    "    ov = embeddings_overlap(list1, list2, depth, index2word, word2vec)[0]\n",
    "    return ov\n",
    "\n",
    "\n",
    "def agreement(list1, list2, depth, index2word, word2vec):\n",
    "    \"\"\"Proportion of shared values between two sorted lists at given depth.\"\"\"\n",
    "    len_intersection, len_set1, len_set2 = embeddings_overlap(list1, list2, depth, index2word, word2vec)\n",
    "    return 2 * len_intersection / (len_set1 + len_set2)\n",
    "\n",
    "\n",
    "def cumulative_agreement(list1, list2, depth, index2word, word2vec):\n",
    "    return (agreement(list1, list2, d, index2word, word2vec) for d in range(1, depth + 1))\n",
    "\n",
    "\n",
    "def average_overlap(list1, list2, index2word, word2vec, depth=None):\n",
    "    \"\"\"Calculate average overlap between ``list1`` and ``list2``.\n",
    "    \"\"\"\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    return sum(cumulative_agreement(list1, list2, depth, index2word=index2word, word2vec=word2vec)) / depth\n",
    "\n",
    "\n",
    "def rbo_at_k(list1, list2, p, index2word, word2vec, depth=None):\n",
    "    # ``p**d`` here instead of ``p**(d - 1)`` because enumerate starts at\n",
    "    # 0\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    d_a = enumerate(cumulative_agreement(list1, list2, depth, index2word=index2word, word2vec=word2vec))\n",
    "    return (1 - p) * sum(p ** d * a for (d, a) in d_a)\n",
    "\n",
    "\n",
    "def rbo_min(list1, list2, p, index2word, word2vec, depth=None):\n",
    "    \"\"\"Tight lower bound on RBO.\n",
    "    See equation (11) in paper.\n",
    "    \"\"\"\n",
    "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
    "    x_k = overlap(list1, list2, depth, index2word, word2vec)\n",
    "    log_term = x_k * math.log(1 - p)\n",
    "    sum_term = sum(\n",
    "        p ** d / d * (overlap(list1, list2, d, index2word, word2vec=word2vec) - x_k) for d in range(1, depth + 1)\n",
    "    )\n",
    "    return (1 - p) / p * (sum_term - log_term)\n",
    "\n",
    "\n",
    "def rbo_res(list1, list2, p, index2word, word2vec):\n",
    "    \"\"\"Upper bound on residual overlap beyond evaluated depth.\n",
    "    See equation (30) in paper.\n",
    "    NOTE: The doctests weren't verified against manual computations but seem\n",
    "    plausible. In particular, for identical lists, ``rbo_min()`` and\n",
    "    ``rbo_res()`` should add up to 1, which is the case.\n",
    "    \"\"\"\n",
    "    S, L = sorted((list1, list2), key=len)\n",
    "    s, l = len(S), len(L)\n",
    "    x_l = overlap(list1, list2, l, index2word, word2vec)\n",
    "    # since overlap(...) can be fractional in the general case of ties and f\n",
    "    # must be an integer --> math.ceil()\n",
    "    f = int(math.ceil(l + s - x_l))\n",
    "    # upper bound of range() is non-inclusive, therefore + 1 is needed\n",
    "    term1 = s * sum(p ** d / d for d in range(s + 1, f + 1))\n",
    "    term2 = l * sum(p ** d / d for d in range(l + 1, f + 1))\n",
    "    term3 = x_l * (math.log(1 / (1 - p)) - sum(p ** d / d for d in range(1, f + 1)))\n",
    "    return p ** s + p ** l - p ** f - (1 - p) / p * (term1 + term2 + term3)\n",
    "\n",
    "\n",
    "def rbo_ext(list1, list2, p, index2word, word2vec):\n",
    "    \"\"\"RBO point estimate based on extrapolating observed overlap.\n",
    "    See equation (32) in paper.\n",
    "    NOTE: The doctests weren't verified against manual computations but seem\n",
    "    plausible.\n",
    "    >>> _round(rbo_ext(\"abcdefg\", \"abcdefg\", .9))\n",
    "    1.0\n",
    "    >>> _round(rbo_ext(\"abcdefg\", \"bacdefg\", .9))\n",
    "    0.9\n",
    "    \"\"\"\n",
    "    S, L = sorted((list1, list2), key=len)\n",
    "    s, l = len(S), len(L)\n",
    "    x_l = overlap(list1, list2, l, index2word, word2vec)\n",
    "    x_s = overlap(list1, list2, s, index2word, word2vec)\n",
    "    # the paper says overlap(..., d) / d, but it should be replaced by\n",
    "    # agreement(..., d) defined as per equation (28) so that ties are handled\n",
    "    # properly (otherwise values > 1 will be returned)\n",
    "    # sum1 = sum(p**d * overlap(list1, list2, d)[0] / d for d in range(1, l + 1))\n",
    "    sum1 = sum(p ** d * agreement(list1, list2, d, index2word=index2word, word2vec=word2vec)\n",
    "               for d in range(1, l + 1))\n",
    "    sum2 = sum(p ** d * x_s * (d - s) / s / d for d in range(s + 1, l + 1))\n",
    "    term1 = (1 - p) / p * (sum1 + sum2)\n",
    "    term2 = p ** l * ((x_l - x_s) / l + x_s / s)\n",
    "    return term1 + term2\n",
    "\n",
    "\n",
    "def word_embeddings_rbo(list1, list2, p, index2word, word2vec):\n",
    "    \"\"\"Complete RBO analysis (lower bound, residual, point estimate).\n",
    "    ``list`` arguments should be already correctly sorted iterables and each\n",
    "    item should either be an atomic value or a set of values tied for that\n",
    "    rank. ``p`` is the probability of looking for overlap at rank k + 1 after\n",
    "    having examined rank k.\n",
    "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
    "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
    "    >>> _round(rbo(lst1, lst2, p=.9))\n",
    "    RBO(min=0.489, res=0.477, ext=0.967)\n",
    "    \"\"\"\n",
    "    if not 0 <= p <= 1:\n",
    "        raise ValueError(\"The ``p`` parameter must be between 0 and 1.\")\n",
    "    args = (list1, list2, p, index2word, word2vec)\n",
    "\n",
    "    return RBO(rbo_min(*args), rbo_res(*args), rbo_ext(*args))\n",
    "\n",
    "\n",
    "def sort_dict(dct, *, ascending=False):\n",
    "    \"\"\"Sort keys in ``dct`` according to their corresponding values.\n",
    "    Sorts in descending order by default, because the values are\n",
    "    typically scores, i.e. the higher the better. Specify\n",
    "    ``ascending=True`` if the values are ranks, or some sort of score\n",
    "    where lower values are better.\n",
    "    Ties are handled by creating sets of tied keys at the given position\n",
    "    in the sorted list.\n",
    "    >>> dct = dict(a=1, b=2, c=1, d=3)\n",
    "    >>> list(sort_dict(dct)) == ['d', 'b', {'a', 'c'}]\n",
    "    True\n",
    "    >>> list(sort_dict(dct, ascending=True)) == [{'a', 'c'}, 'b', 'd']\n",
    "    True\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    items = []\n",
    "    # items should be unique, scores don't have to\n",
    "    for item, score in dct.items():\n",
    "        if not ascending:\n",
    "            score *= -1\n",
    "        i = bisect_left(scores, score)\n",
    "        if i == len(scores):\n",
    "            scores.append(score)\n",
    "            items.append(item)\n",
    "        elif scores[i] == score:\n",
    "            existing_item = items[i]\n",
    "            if isinstance(existing_item, set):\n",
    "                existing_item.add(item)\n",
    "            else:\n",
    "                items[i] = {existing_item, item}\n",
    "        else:\n",
    "            scores.insert(i, score)\n",
    "            items.insert(i, item)\n",
    "    return items\n",
    "\n",
    "\n",
    "def rbo_dict(dict1, dict2, p, index2word, word2vec, *, sort_ascending=False):\n",
    "    \"\"\"Wrapper around ``rbo()`` for dict input.\n",
    "    Each dict maps items to be sorted to the score according to which\n",
    "    they should be sorted. The RBO analysis is then performed on the\n",
    "    resulting sorted lists.\n",
    "    The sort is descending by default, because scores are typically the\n",
    "    higher the better, but this can be overridden by specifying\n",
    "    ``sort_ascending=True``.\n",
    "    \"\"\"\n",
    "    list1, list2 = (\n",
    "        sort_dict(dict1, ascending=sort_ascending),\n",
    "        sort_dict(dict2, ascending=sort_ascending),\n",
    "    )\n",
    "    return word_embeddings_rbo(list1, list2, p, index2word, word2vec)\n",
    "\n",
    "def get_diversity_scores(topics, model, topn=10, topic_type=''):\n",
    "    df = pd.DataFrame({\"puw:\":proportion_unique_words(topics, topk=topn),\n",
    "\"jd:\": pairwise_jaccard_diversity(topics, topk=topn),\n",
    "\"we-pd:\": pairwise_word_embedding_distance(topics, model, topk=topn),\n",
    "\"we-cd:\": centroid_distance(topics, model.wv, topk=topn),\n",
    "\"we-irbo p=0.5:\":word_embedding_irbo(topics,model.wv, weight=0.5, topk=topn),\n",
    "\"we-irbo p=0.9:\":word_embedding_irbo(topics,model.wv, weight=0.9, topk=topn)},index=[topic_type])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UrYBIBXbxaRn"
   },
   "outputs": [],
   "source": [
    "citydata_df = pd.read_parquet('citydata.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = pd.read_parquet('citydata-covid.parquet')\n",
    "crime = pd.read_parquet('citydata-crime.parquet')\n",
    "metro = pd.read_parquet('citydata-metro.parquet')\n",
    "plan = pd.read_parquet('citydata-2035plan.parquet')\n",
    "retail = pd.read_parquet('citydata-retail.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO = make_graph_citydata(covid)\n",
    "CR = make_graph_citydata(crime)\n",
    "ME = make_graph_citydata(metro)\n",
    "PL = make_graph_citydata(plan)\n",
    "R = make_graph_citydata(retail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citydata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_posts = [covid['post'].map(lambda x: re.subn(r'Advertisements','',x)[0])[0],\n",
    "crime['post'].map(lambda x: re.subn(r'Advertisements','',x)[0])[0] + ' ' + metro['post'].map(lambda x: re.subn(r'Advertisements','',x)[0])[0],\n",
    "plan['post'].map(lambda x: re.subn(r'Advertisements','',x)[0])[0],\n",
    "retail['post'].map(lambda x: re.subn(r'Advertisements','',x)[0])[0],]\n",
    "\n",
    "#first_posts_embeddings = smodel.encode(first_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_labels = ['covid' for i in range(len(covid))]\n",
    "crime_labels = ['covid' for i in range(len(crime))]\n",
    "metro_labels = ['covid' for i in range(len(metro))]\n",
    "plan_labels = ['covid' for i in range(len(plan))]\n",
    "retail_labels = ['covid' for i in range(len(retail))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY, city_paths, city_chains, city_embeddings, city_singletons,city_singleton_embeddings, city_singleton_texts = make_thread_embeddings(citydata_df,smodel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle([CITY, city_paths, city_chains, city_embeddings, city_singletons,city_singleton_embeddings, city_singleton_texts],'citydata_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = open_pickle('citydata_processed.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY = city[0]\n",
    "city_paths = city[1]\n",
    "city_chains = city[2]\n",
    "city_embeddings = city[3]\n",
    "city_singletons = city[4]\n",
    "city_singleton_embeddings = city[5]\n",
    "city_singleton_texts = city[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\bertopic\\lib\\site-packages\\scipy\\spatial\\distance.py:622: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  u_mass_coherence   puw:       jd:    we-pd:   we-cd:  \\\n",
      "BERTopic Threads         -5.591325  0.496  0.795484  0.462137  0.28213   \n",
      "\n",
      "                  we-irbo p=0.5:  we-irbo p=0.9:  \n",
      "BERTopic Threads        0.624762         0.53839  \n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# seed_topic_list = [[\"drug\", \"cancer\", \"drugs\", \"doctor\"],\n",
    "#                    [\"windows\", \"drive\", \"dos\", \"file\"],\n",
    "#                    [\"space\", \"launch\", \"orbit\", \"lunar\"]]\n",
    "\n",
    "\n",
    "\n",
    "# Train BERTopic with a custom CountVectorizer\n",
    "vectorizer_model = CountVectorizer(min_df=3, stop_words='english')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=50, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model = BERTopic(hdbscan_model=hdbscan_model, vectorizer_model=vectorizer_model,nr_topics=5, top_n_words=25)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(threads)\n",
    "\n",
    "tw = [[w for (w,s) in topic_model.get_topic(-1)],\n",
    "      [w for (w,s) in topic_model.get_topic(0)], \n",
    "      [w for (w,s) in topic_model.get_topic(1)],  \n",
    "      [w for (w,s) in topic_model.get_topic(2)],\n",
    "     [w for (w,s) in topic_model.get_topic(3)],]\n",
    "#print(get_topical_coherence(threads, tw,metric='u_mass'))\n",
    "print(get_coherence_diversity_scores(threads, tw,fast,topn=25,topic_type='BERTopic Threads'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>city</td>\n",
       "      <td>like</td>\n",
       "      <td>people</td>\n",
       "      <td>philly</td>\n",
       "      <td>just</td>\n",
       "      <td>think</td>\n",
       "      <td>dont</td>\n",
       "      <td>philadelphia</td>\n",
       "      <td>new</td>\n",
       "      <td>street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>city</td>\n",
       "      <td>like</td>\n",
       "      <td>new</td>\n",
       "      <td>store</td>\n",
       "      <td>think</td>\n",
       "      <td>just</td>\n",
       "      <td>philadelphia</td>\n",
       "      <td>retail</td>\n",
       "      <td>stores</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people</td>\n",
       "      <td>crime</td>\n",
       "      <td>city</td>\n",
       "      <td>dont</td>\n",
       "      <td>just</td>\n",
       "      <td>like</td>\n",
       "      <td>white</td>\n",
       "      <td>im</td>\n",
       "      <td>know</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hmmm</td>\n",
       "      <td>hello</td>\n",
       "      <td>fancy</td>\n",
       "      <td>update</td>\n",
       "      <td>haha</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inga</td>\n",
       "      <td>like</td>\n",
       "      <td>just</td>\n",
       "      <td>news</td>\n",
       "      <td>article</td>\n",
       "      <td>dont</td>\n",
       "      <td>read</td>\n",
       "      <td>writing</td>\n",
       "      <td>people</td>\n",
       "      <td>does</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1       2       3        4      5             6             7  \\\n",
       "0    city   like  people  philly     just  think          dont  philadelphia   \n",
       "1    city   like     new   store    think   just  philadelphia        retail   \n",
       "2  people  crime    city    dont     just   like         white            im   \n",
       "3    hmmm  hello   fancy  update     haha                                      \n",
       "4    inga   like    just    news  article   dont          read       writing   \n",
       "\n",
       "        8       9  \n",
       "0     new  street  \n",
       "1  stores  people  \n",
       "2    know    year  \n",
       "3                  \n",
       "4  people    does  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tw = [[w for (w,s) in topic_model.get_topic(-1)],\n",
    "      [w for (w,s) in topic_model.get_topic(0)], \n",
    "      [w for (w,s) in topic_model.get_topic(1)],  \n",
    "      [w for (w,s) in topic_model.get_topic(2)],\n",
    "     [w for (w,s) in topic_model.get_topic(3)],]\n",
    "\n",
    "pd.DataFrame(tw)[[0,1,2,3,4,5,6,7,8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>4639</td>\n",
       "      <td>-1_city_like_people_philly</td>\n",
       "      <td>[city, like, people, philly, just, think, dont...</td>\n",
       "      <td>[ What makes a city world class: History, tour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6363</td>\n",
       "      <td>0_city_like_new_store</td>\n",
       "      <td>[city, like, new, store, think, just, philadel...</td>\n",
       "      <td>[ The Country Club Plaza is generally consider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2804</td>\n",
       "      <td>1_people_crime_city_dont</td>\n",
       "      <td>[people, crime, city, dont, just, like, white,...</td>\n",
       "      <td>[ What is going on with this forum and the may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>701</td>\n",
       "      <td>2_hmmm_hello_fancy_update</td>\n",
       "      <td>[hmmm, hello, fancy, update, haha, , , , , , ,...</td>\n",
       "      <td>[ haha ,  Update.,  Fancy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>501</td>\n",
       "      <td>3_inga_like_just_news</td>\n",
       "      <td>[inga, like, just, news, article, dont, read, ...</td>\n",
       "      <td>[ She's the \"architecture\" critic for the Inqu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                        Name  \\\n",
       "0     -1   4639  -1_city_like_people_philly   \n",
       "1      0   6363       0_city_like_new_store   \n",
       "2      1   2804    1_people_crime_city_dont   \n",
       "3      2    701   2_hmmm_hello_fancy_update   \n",
       "4      3    501       3_inga_like_just_news   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [city, like, people, philly, just, think, dont...   \n",
       "1  [city, like, new, store, think, just, philadel...   \n",
       "2  [people, crime, city, dont, just, like, white,...   \n",
       "3  [hmmm, hello, fancy, update, haha, , , , , , ,...   \n",
       "4  [inga, like, just, news, article, dont, read, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [ What makes a city world class: History, tour...  \n",
       "1  [ The Country Club Plaza is generally consider...  \n",
       "2  [ What is going on with this forum and the may...  \n",
       "3                         [ haha ,  Update.,  Fancy]  \n",
       "4  [ She's the \"architecture\" critic for the Inqu...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>10408</td>\n",
       "      <td>-1_city_like_people_just</td>\n",
       "      <td>[city, like, people, just, philly, philadelphi...</td>\n",
       "      <td>[ In my opinion, you want to turn Philadelphia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8720</td>\n",
       "      <td>0_city_like_think_new</td>\n",
       "      <td>[city, like, think, new, just, people, buildin...</td>\n",
       "      <td>[ i am not trying to be a hater, but that is U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2591</td>\n",
       "      <td>1_white_people_city_dont</td>\n",
       "      <td>[white, people, city, dont, like, crime, im, y...</td>\n",
       "      <td>[ I definitely agree that Millennials have to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1028</td>\n",
       "      <td>2_like_inga_news_just</td>\n",
       "      <td>[like, inga, news, just, dont, people, article...</td>\n",
       "      <td>[ In her typical hysterical fashion, Inga is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>705</td>\n",
       "      <td>3_bau_hello_nasty_bart</td>\n",
       "      <td>[bau, hello, nasty, bart, fancy, update, haha,...</td>\n",
       "      <td>[ Fancy, Nasty., bau?]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                      Name  \\\n",
       "0     -1  10408  -1_city_like_people_just   \n",
       "1      0   8720     0_city_like_think_new   \n",
       "2      1   2591  1_white_people_city_dont   \n",
       "3      2   1028     2_like_inga_news_just   \n",
       "4      3    705    3_bau_hello_nasty_bart   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [city, like, people, just, philly, philadelphi...   \n",
       "1  [city, like, think, new, just, people, buildin...   \n",
       "2  [white, people, city, dont, like, crime, im, y...   \n",
       "3  [like, inga, news, just, dont, people, article...   \n",
       "4  [bau, hello, nasty, bart, fancy, update, haha,...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [ In my opinion, you want to turn Philadelphia...  \n",
       "1  [ i am not trying to be a hater, but that is U...  \n",
       "2  [ I definitely agree that Millennials have to ...  \n",
       "3  [ In her typical hysterical fashion, Inga is a...  \n",
       "4                             [ Fancy, Nasty., bau?]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\bertopic\\lib\\site-packages\\scipy\\spatial\\distance.py:622: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                u_mass_coherence  puw:       jd:    we-pd:    we-cd:  \\\n",
      "BERTopic Posts         -5.121572  0.52  0.780486  0.522732  0.231391   \n",
      "\n",
      "                we-irbo p=0.5:  we-irbo p=0.9:  \n",
      "BERTopic Posts        0.587038        0.495225  \n"
     ]
    }
   ],
   "source": [
    "# Train BERTopic with a custom CountVectorizer\n",
    "post_vectorizer_model = CountVectorizer(min_df=1, stop_words='english')\n",
    "post_hdbscan_model = HDBSCAN(min_cluster_size=50, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "post_topic_model = BERTopic(hdbscan_model=post_hdbscan_model, vectorizer_model=post_vectorizer_model,nr_topics=5,top_n_words=25)\n",
    "\n",
    "post_topics, post_probs = post_topic_model.fit_transform(posts_quotes)\n",
    "\n",
    "tw = [[w for (w,s) in post_topic_model.get_topic(-1)],\n",
    "      [w for (w,s) in post_topic_model.get_topic(0)], \n",
    "      [w for (w,s) in post_topic_model.get_topic(1)],  \n",
    "      [w for (w,s) in post_topic_model.get_topic(2)],\n",
    "     [w for (w,s) in post_topic_model.get_topic(3)],]\n",
    "#print(get_topical_coherence(posts_quotes, tw,metric='u_mass'))\n",
    "print(get_coherence_diversity_scores(threads, tw,fast,topn=25,topic_type='BERTopic Posts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = [[w for (w,s) in post_topic_model.get_topic(-1)],\n",
    "      [w for (w,s) in post_topic_model.get_topic(0)], \n",
    "      [w for (w,s) in post_topic_model.get_topic(1)],  \n",
    "      [w for (w,s) in post_topic_model.get_topic(2)],\n",
    "     [w for (w,s) in post_topic_model.get_topic(3)],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>city</td>\n",
       "      <td>like</td>\n",
       "      <td>people</td>\n",
       "      <td>just</td>\n",
       "      <td>philly</td>\n",
       "      <td>philadelphia</td>\n",
       "      <td>think</td>\n",
       "      <td>dont</td>\n",
       "      <td>street</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>city</td>\n",
       "      <td>like</td>\n",
       "      <td>think</td>\n",
       "      <td>new</td>\n",
       "      <td>just</td>\n",
       "      <td>people</td>\n",
       "      <td>building</td>\n",
       "      <td>philadelphia</td>\n",
       "      <td>dont</td>\n",
       "      <td>philly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white</td>\n",
       "      <td>people</td>\n",
       "      <td>city</td>\n",
       "      <td>dont</td>\n",
       "      <td>like</td>\n",
       "      <td>crime</td>\n",
       "      <td>im</td>\n",
       "      <td>year</td>\n",
       "      <td>just</td>\n",
       "      <td>murders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>inga</td>\n",
       "      <td>news</td>\n",
       "      <td>just</td>\n",
       "      <td>dont</td>\n",
       "      <td>people</td>\n",
       "      <td>article</td>\n",
       "      <td>think</td>\n",
       "      <td>does</td>\n",
       "      <td>thread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bau</td>\n",
       "      <td>hello</td>\n",
       "      <td>nasty</td>\n",
       "      <td>bart</td>\n",
       "      <td>fancy</td>\n",
       "      <td>update</td>\n",
       "      <td>haha</td>\n",
       "      <td>awful</td>\n",
       "      <td>ok</td>\n",
       "      <td>finally</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2     3       4             5         6             7  \\\n",
       "0   city    like  people  just  philly  philadelphia     think          dont   \n",
       "1   city    like   think   new    just        people  building  philadelphia   \n",
       "2  white  people    city  dont    like         crime        im          year   \n",
       "3   like    inga    news  just    dont        people   article         think   \n",
       "4    bau   hello   nasty  bart   fancy        update      haha         awful   \n",
       "\n",
       "        8        9  \n",
       "0  street   center  \n",
       "1    dont   philly  \n",
       "2    just  murders  \n",
       "3    does   thread  \n",
       "4      ok  finally  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tw)[[0,1,2,3,4,5,6,7,8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_nodes = [(text_process(CO.nodes()[k]['text']).split(),k) for (k,v) in sorted(dict(CO.degree()).items(), key=lambda x:x[1],reverse=True)][0]\n",
    "crime_nodes = [(text_process(CR.nodes()[k]['text']).split(),k) for (k,v) in sorted(dict(CR.degree()).items(), key=lambda x:x[1],reverse=True)][0]\n",
    "metro_nodes = [(text_process(ME.nodes()[k]['text']).split(),k) for (k,v) in sorted(dict(ME.degree()).items(), key=lambda x:x[1],reverse=True)][0]\n",
    "plan_nodes = [(text_process(PL.nodes()[k]['text']).split(),k) for (k,v) in sorted(dict(PL.degree()).items(), key=lambda x:x[1],reverse=True)][0]\n",
    "retail_nodes = [(text_process(R.nodes()[k]['text']).split(),k) for (k,v) in sorted(dict(R.degree()).items(), key=lambda x:x[1],reverse=True)][0]\n",
    "\n",
    "\n",
    "#degree_seed_embeddings = smodel.encode([covid_nodes,crime_nodes + metro_nodes, plan_nodes, retail_nodes])\n",
    "degree_seeds = [covid_nodes,crime_nodes,metro_nodes, plan_nodes, retail_nodes]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# seed_topic_list = [d[0][:10] for d in degree_seeds]\n",
    "\n",
    "\n",
    "seed_topic_list = [[\"drug\", \"cancer\", \"drugs\", \"doctor\"],\n",
    "                   [\"windows\", \"drive\", \"dos\", \"file\"],\n",
    "                   [\"space\", \"launch\", \"orbit\", \"lunar\"]]\n",
    "\n",
    "degree_vectorizer_model = CountVectorizer(min_df=1, stop_words='english')\n",
    "degree_hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "degree_topic_model = BERTopic(seed_topic_list=seed_topic_list,hdbscan_model=degree_hdbscan_model, vectorizer_model=degree_vectorizer_model,nr_topics=5,top_n_words=25)\n",
    "\n",
    "degree_topics, degree_probs = degree_topic_model.fit_transform(posts_quotes)\n",
    "\n",
    "tw = [[w for (w,s) in post_topic_model.get_topic(-1)],\n",
    "      [w for (w,s) in post_topic_model.get_topic(0)], \n",
    "      [w for (w,s) in post_topic_model.get_topic(1)],  \n",
    "      [w for (w,s) in post_topic_model.get_topic(2)],\n",
    "     [w for (w,s) in post_topic_model.get_topic(3)],]\n",
    "print(get_topical_coherence(posts_quotes, tw,metric='u_mass'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#city_embeddings = np.concatenate([city_embeddings, city_singleton_embeddings])\n",
    "threads = city_chains + city_singleton_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_threads = [text_process(thread) for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('57681616', 5)\n",
      "('50175634', 5)\n",
      "('22518391', 5)\n",
      "('38332691', 5)\n",
      "('38055575', 5)\n"
     ]
    }
   ],
   "source": [
    "print([(k,v) for (k,v) in sorted(dict(CO.degree()).items(), key=lambda x:x[1],reverse=True)][0])\n",
    "print([(k,v) for (k,v) in sorted(dict(CR.degree()).items(), key=lambda x:x[1],reverse=True)][0])\n",
    "print([(k,v) for (k,v) in sorted(dict(ME.degree()).items(), key=lambda x:x[1],reverse=True)][0])\n",
    "print([(k,v) for (k,v) in sorted(dict(PL.degree()).items(), key=lambda x:x[1],reverse=True)][0])\n",
    "print([(k,v) for (k,v) in sorted(dict(R.degree()).items(), key=lambda x:x[1],reverse=True)][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21228"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(posts_quotes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Rittenhouse:Not one, but TWO 'hood' mobs attacking innocents in one of the best neighborhoods in the city. Real nice.We all know the demographics of said mobs, I just wish the media and law enforcement would say it.\",\n",
       " '50175634')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rotodome made a statement that you had a better chance to die in an auto accident then to be murdered in Philly. The stats in 2010 are as follows..People killed in auto accidents in the city- 84people murdered in the city- 306 (2011 was 324)so to sum it up. You are approximately 4 times as likely to be murdered in Philly then to die in an auto accident.. Interesting..',\n",
       " '22518391')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metro_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Well on the W lot at 15th and Chestnut is now permanently closed - fencing is up and an under progress sign with the W render now hungwow',\n",
       " '38332691')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This is what I got so far:1. CHIPOTLE Chipotle is coming to Liberty Place. If you go to the food court, you will see a boarded-up area near the corner across-ish from Chic-Fil-A. 2. INDOCHINO Just across from Liberty Place, Indochino is officially opened. It is located to the left of UNIQLO. It used to be occupied by Liberty Travel.3. LIBERTY TRAVEL Speaking of Liberty Travel, they moved one block away, closer to Modell\\'s. I personally love how they make the building kinda chic, clean and attractive from outside. There is a huge monitor display inside that made people turned their heads around to see what\\'s up. And on the top of that, this means one more empty storefront is filled. YAY!4. NEW BALANCE I heard New Balance concept store is coming to Walnut this Summer (before Under Armour!) How stealthy! 5. BANANA REPUBLIC FACTORY STORE AND GAP OUTLET STORE Banana republic Factory store has been opened for 3 weeks now on Chestnut. And I am glad the GAP outlet across the street from it is not closed. I heard rumor that it will be closed, leaving a visible strip of blight-ness, combining that with Boyd Theatre. I talked to the manager there, and they were like \"What? WHo? We are closing? Said who?\" Well at least that is what she said. And the rumor was that it would be closed on Dec 2014. As up to last night when I went home, it was still there opened. Here hoping that it would not be closed.6. MAIN FOOD COURT OF THE GALLERY MARKET EAST Switching to the Gallery. Most of the food vendors in the main food court had been gone. IT is so sad because I love KFC. But then when I went to Books-A-Million, the manager told me that all the food vendors will be relocated toward their direction, ie closer to C21. I have no idea how it will be done. The only thing I can imagine is that, it will occupy that left area of the corridor to El. That closer area that is currently closed with glass(?) walls plastered with C21 ads.And for Books-a-Million itself, they have not heard any news from their upper managament when they will be moved. My guess is that PREIT will just close the main food court area, leaving the wings opened for retailers to congregate, Of course in the process, it looks like Modell\\'s has to be closed as well. 7. BARNES AND NOBLES NEAR DILWORTH PARKIt is not opened yet! (I know this is non-news). They promised it will be opened this month! I am still crossing my fingers.8. TOP SHOP / TOP MAN????Just some article I read about East Market. So this is just from an article I read.Aside of MOM\\'s, according to an article they are scouting to have another UNIQLO and wait for it....(gasp) TOP SHOP / TOP MAN! hjkjhdskjfhgsdgss! YAYYYYY!!!!! We all know how lame / incomplete their collection is in Nordstrom KOP! (Sorry Nordstrom!)9. G-STAR AND / OR SUPERDRY????I talked to the owner of G-Star (apparently he bought it as a franchise??) in Cherry Hill Mall; he was interested in the SuperDry jacket I wore and whereabout I got it. The way we conversed at the end sounded to me as though he was surveying what retails people in CC want / need. They said that they tried to find a location in Center City for possibly G-star and/or SuperDry. I literally begged him to open a G-Star and / or Super Dry on Walnut. So it seems to me that the dude is STILL looking to open a store somewhere in CC. Maybe he will do it in East Market development.',\n",
       " '38055575')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retail_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23058"
      ]
     },
     "execution_count": 883,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(citydata_df.post_id.tolist() + citydata_df.quote_id.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles= \"\"\"How's everyone doing amongst the Coronavirus shut down?\n",
    "Official Greater Philadelphia Area Crime Thread (Chester, New Castle: 2013, middle school, university) Official Philadelphia Metro Crime Thread (York, Chester: apartment complexes, houses, unemployment) \n",
    "Philadelphia 2035 (Houston: foreclosure, neighborhoods, wage) \n",
    "Retail coming to Philadelphia (Penn, Burlington: real estate, house, buying)\"\"\"\n",
    "\n",
    "title_embeddings = smodel.encode(titles.split('\\n'))\n",
    "\n",
    "title_terms = [text_tokenize(title) for title in titles.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.476321141738323\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df0 = cluster_thread_embeddings_kmeans_tfidf(threads,\n",
    "                                             city_embeddings,\n",
    "                                             vectorizer='tfidf',\n",
    "                                             mindf=10,\n",
    "                                             clusters=4,ngrams=(1,2),\n",
    "                                             kbest=7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.457882055127163\n"
     ]
    }
   ],
   "source": [
    "#no priors thread level\n",
    "\n",
    "df1 = cluster_thread_embeddings_kmeans_tfidf(threads,\n",
    "                                                                                                                           city_embeddings,\n",
    "                                                                       mindf=10,\n",
    "                                                                                    clusters=4,ngrams=(1,3),\n",
    "                                                                                 kbest=7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cluster centers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1362: RuntimeWarning:\n",
      "\n",
      "Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.266071728654636\n"
     ]
    }
   ],
   "source": [
    "#cluster center priors from high degree nodes\n",
    "\n",
    "\n",
    "df2 = cluster_thread_embeddings_kmeans_tfidf(threads, \n",
    "                                             city_embeddings,\n",
    "                                             clusters=4,\n",
    "                                             ngrams=(1,3),\n",
    "                                             mindf=10,\n",
    "                                             kbest=7500,init_=degree_seed_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cluster centers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1362: RuntimeWarning:\n",
      "\n",
      "Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.57219736559069\n"
     ]
    }
   ],
   "source": [
    "#cluster center priors from first post\n",
    "#threads\n",
    "\n",
    "\n",
    "df3 = cluster_thread_embeddings_kmeans_tfidf(threads, \n",
    "                                            city_embeddings,\n",
    "                                            mindf=10,\n",
    "                                            clusters=4,\n",
    "                                            ngrams=(1,3),\n",
    "                                            kbest=7500,init_=first_posts_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_quotes = citydata_df.post.tolist() + [quote for quote in citydata_df.quote.tolist() if quote not in citydata_df.post.tolist()]\n",
    "#post_embeddings = smodel.encode(posts_quotes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.7146329029264553\n"
     ]
    }
   ],
   "source": [
    "#post-level embeddings no priors\n",
    "df4 = cluster_thread_embeddings_kmeans_tfidf(posts_quotes, \n",
    "                                                                                                                           post_embeddings,\n",
    "                                                                       mindf=10,\n",
    "                                                                                  clusters=4,ngrams=(1,3),\n",
    "                                                                                 kbest=7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cluster centers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1362: RuntimeWarning:\n",
      "\n",
      "Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.562278630401623\n"
     ]
    }
   ],
   "source": [
    "#posts_quotes = citydata_df.post.tolist() + [quote for quote in citydata_df.quote.tolist() if quote not in city_data_df.post.tolist()]\n",
    "#post_embeddings = smodel.encode(posts_quotes)\n",
    "#post-level topics with init_posts as centers\n",
    "df5 = cluster_thread_embeddings_kmeans_tfidf(posts_quotes, \n",
    "                                                                                                                           post_embeddings,\n",
    "                                                                       mindf=10,\n",
    "                                                                                    clusters=4,ngrams=(1,3),\n",
    "                                                                                 kbest=7500,init_=first_posts_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.288412726815769\n"
     ]
    }
   ],
   "source": [
    "#post-level embeddings degree seeds\n",
    "df6 = cluster_thread_embeddings_kmeans_tfidf(posts_quotes, \n",
    "                                                                                                                           post_embeddings,\n",
    "                                                                       mindf=10,\n",
    "                                                                                    clusters=4,ngrams=(1,3),\n",
    "                                                                                 kbest=7500,init_=degree_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=10)\n",
    "v = tsne.fit_transform(city_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from plotly.offline import iplot\n",
    "\n",
    "plot(px.scatter(x=v[:,0],y=v[:,1], color=[str(k) for k in km8.labels_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cluster centers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1362: RuntimeWarning:\n",
      "\n",
      "Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.761954646512221\n"
     ]
    }
   ],
   "source": [
    "#post-level embeddings froum title seeds\n",
    "df7 = cluster_thread_embeddings_kmeans_tfidf(posts_quotes, \n",
    "                                                                                    post_embeddings,\n",
    "                                                                       mindf=10,\n",
    "                                                                                    clusters=4,ngrams=(1,3),\n",
    "                                                                                 kbest=7500,init_=title_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cluster centers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n",
      "C:\\Users\\rmomi\\anaconda3\\envs\\ktransformers\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1362: RuntimeWarning:\n",
      "\n",
      "Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.842381752972228\n"
     ]
    }
   ],
   "source": [
    "#thread-level embedding forum title seeds\n",
    "df8 = cluster_thread_embeddings_kmeans_tfidf(threads, \n",
    "                                                                                    city_embeddings,\n",
    "                                                                       mindf=10,\n",
    "                                                                                clusters=4,ngrams=(1,3),\n",
    "                                                                                 kbest=7500,init_=title_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['threads', 'term_weights', 'vectorizer', 'clusterer', 'topics'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df8.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['topics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embeddings Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_topics_eval_df = pd.concat([\n",
    "        get_coherence_diversity_scores(df4['threads'].thread, df4['topics'], fast,topn=25,topic_type='tfidf post'),\n",
    "        get_coherence_diversity_scores(df7['threads'].thread, df7['topics'], fast,topn=25,topic_type='guided tfidf (post titles)'),\n",
    "        get_coherence_diversity_scores(df5['threads'].thread, df5['topics'], fast,topn=25,topic_type='guided tfidf (initial posts)'),\n",
    "        get_coherence_diversity_scores(df6['threads'].thread, df6['topics'], fast,topn=25,topic_type='guided tfidf (high degree)'),\n",
    "        get_coherence_diversity_scores(df1['threads'].thread, df1['topics'], fast,topn=25,topic_type='tfidf threads'),\n",
    "        get_coherence_diversity_scores(df8['threads'].thread, df8['topics'], fast,topn=25,topic_type='guided tfidf threads post titles'),\n",
    "        get_coherence_diversity_scores(df3['threads'].thread, df3['topics'], fast,topn=25,topic_type='guided tfidf threads (initial posts)'),\n",
    "        get_coherence_diversity_scores(df2['threads'].thread, df2['topics'], fast,topn=25,topic_type='guided tfidf threads (high degree)'),\n",
    "        \n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_mass_coherence</th>\n",
       "      <th>puw:</th>\n",
       "      <th>jd:</th>\n",
       "      <th>we-cd:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tfidf post</th>\n",
       "      <td>-3.714633</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.851137</td>\n",
       "      <td>0.101701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf (post titles)</th>\n",
       "      <td>-2.761955</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.638784</td>\n",
       "      <td>0.065617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf (initial posts)</th>\n",
       "      <td>-2.562279</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.620674</td>\n",
       "      <td>0.071358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf (high degree)</th>\n",
       "      <td>-3.288413</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.857064</td>\n",
       "      <td>0.101701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf threads</th>\n",
       "      <td>-4.457882</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.855737</td>\n",
       "      <td>0.234158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf threads post titles</th>\n",
       "      <td>-4.842382</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.797508</td>\n",
       "      <td>0.065617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf threads (initial posts)</th>\n",
       "      <td>-4.572197</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.785500</td>\n",
       "      <td>0.218687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf threads (high degree)</th>\n",
       "      <td>-4.266072</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.785500</td>\n",
       "      <td>0.216487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      u_mass_coherence  puw:       jd:  \\\n",
       "tfidf post                                   -3.714633  0.73  0.851137   \n",
       "guided tfidf (post titles)                   -2.761955  0.54  0.638784   \n",
       "guided tfidf (initial posts)                 -2.562279  0.52  0.620674   \n",
       "guided tfidf (high degree)                   -3.288413  0.74  0.857064   \n",
       "tfidf threads                                -4.457882  0.73  0.855737   \n",
       "guided tfidf threads post titles             -4.842382  0.69  0.797508   \n",
       "guided tfidf threads (initial posts)         -4.572197  0.67  0.785500   \n",
       "guided tfidf threads (high degree)           -4.266072  0.67  0.785500   \n",
       "\n",
       "                                        we-cd:  \n",
       "tfidf post                            0.101701  \n",
       "guided tfidf (post titles)            0.065617  \n",
       "guided tfidf (initial posts)          0.071358  \n",
       "guided tfidf (high degree)            0.101701  \n",
       "tfidf threads                         0.234158  \n",
       "guided tfidf threads post titles      0.065617  \n",
       "guided tfidf threads (initial posts)  0.218687  \n",
       "guided tfidf threads (high degree)    0.216487  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "se_topics_eval_df[['u_mass_coherence','puw:','jd:','we-cd:']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u_mass_coherence   -3.808226\n",
       "puw:                0.661250\n",
       "jd:                 0.773988\n",
       "we-pd:              0.667342\n",
       "we-cd:              0.134416\n",
       "we-irbo p=0.5:      0.600381\n",
       "we-irbo p=0.9:      0.522834\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_topics_eval_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>we-cd:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tfidf post</th>\n",
       "      <td>0.440207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf (post titles)</th>\n",
       "      <td>0.097374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf (initial posts)</th>\n",
       "      <td>0.085735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf (high degree)</th>\n",
       "      <td>0.085735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf threads</th>\n",
       "      <td>0.215439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf threads post titles</th>\n",
       "      <td>0.092573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf threads (initial posts)</th>\n",
       "      <td>0.185650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided tfidf threads (high degree)</th>\n",
       "      <td>0.232007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        we-cd:\n",
       "tfidf post                            0.440207\n",
       "guided tfidf (post titles)            0.097374\n",
       "guided tfidf (initial posts)          0.085735\n",
       "guided tfidf (high degree)            0.085735\n",
       "tfidf threads                         0.215439\n",
       "guided tfidf threads post titles      0.092573\n",
       "guided tfidf threads (initial posts)  0.185650\n",
       "guided tfidf threads (high degree)    0.232007"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(se_topics_eval_df['we-cd:'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0751703998894153"
      ]
     },
     "execution_count": 847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_topics_eval_df['u_mass_coherence'][:4].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.2605348650148365"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_topics_eval_df['u_mass_coherence'][4:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Post and Thread Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pposts = df7['threads'].thread\n",
    "pthreads = df1['threads'].thread\n",
    "gensim_post_model, gensim_post_terms, gensim_post_corpus, gensim_processed_posts = gensim_lda(pposts, topic_num=4)\n",
    "gensim_thread_model, gensim_thread_terms, gensim_thread_corpus, gensim_processed_threads = gensim_lda(pthreads, topic_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_posts_tokens = [text_tokenize(post) for post in first_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_processed_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Guided Initial Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_init_post_model, gensim_init_posts_terms, gensim_init_posts_corpus, processed_posts_init_posts = gensim_lda(pposts, topic_num=4, topic_word_priors=first_posts_tokens, eta_=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_thread_init_post_model, gensim_thread_init_posts_terms, gensim__thread_init_posts_corpus, gensim_thread_processed_posts_init_posts = gensim_lda(pthreads, topic_num=4, topic_word_priors=first_posts_tokens, eta_=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Guided Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_titles_model, gensim_titles_terms, gensim_titles_corpus, gensim_titles_processed_posts = gensim_lda(pposts, topic_num=4, topic_word_priors=title_terms, eta_=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_thread_titles_model, gensim_thread_titles_terms, gensim_thread_titles_corpus, gensim_thread_titles_processed_posts = gensim_lda(pthreads, topic_num=4, topic_word_priors=title_terms, eta_=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Guided High Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_priors = [text_tokenize(node) for node in [covid_nodes, crime_nodes + metro_nodes, plan_nodes,retail_nodes]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_degree_model, gensim_degree_terms, gensim_degree_corpus, gensim_degree_processed_posts = gensim_lda(pposts, topic_num=4, topic_word_priors=degree_priors, eta_=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_thread_degree_model, gensim__thread_degree_terms, gensim_thread_degree_corpus, gensim_thread_degree_processed_posts = gensim_lda(pthreads, topic_num=4, topic_word_priors=degree_priors, eta_=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([g[:10] for g in topics6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>money</td>\n",
       "      <td>state</td>\n",
       "      <td>local</td>\n",
       "      <td>people</td>\n",
       "      <td>issue</td>\n",
       "      <td>care</td>\n",
       "      <td>neighborhood</td>\n",
       "      <td>income</td>\n",
       "      <td>help</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store</td>\n",
       "      <td>retail</td>\n",
       "      <td>mall</td>\n",
       "      <td>shopping</td>\n",
       "      <td>retailer</td>\n",
       "      <td>location</td>\n",
       "      <td>center</td>\n",
       "      <td>shop</td>\n",
       "      <td>gallery</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>city</td>\n",
       "      <td>think</td>\n",
       "      <td>philadelphia</td>\n",
       "      <td>philly</td>\n",
       "      <td>people</td>\n",
       "      <td>year</td>\n",
       "      <td>time</td>\n",
       "      <td>even</td>\n",
       "      <td>area</td>\n",
       "      <td>much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>street</td>\n",
       "      <td>market</td>\n",
       "      <td>building</td>\n",
       "      <td>walnut</td>\n",
       "      <td>space</td>\n",
       "      <td>chestnut</td>\n",
       "      <td>center</td>\n",
       "      <td>east</td>\n",
       "      <td>block</td>\n",
       "      <td>south</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1             2         3         4         5             6  \\\n",
       "0   money   state         local    people     issue      care  neighborhood   \n",
       "1   store  retail          mall  shopping  retailer  location        center   \n",
       "2    city   think  philadelphia    philly    people      year          time   \n",
       "3  street  market      building    walnut     space  chestnut        center   \n",
       "\n",
       "        7        8      9  \n",
       "0  income     help  white  \n",
       "1    shop  gallery   also  \n",
       "2    even     area   much  \n",
       "3    east    block  south  "
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([g[:10] for g in gensim_degree_terms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_degree_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23436"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20582"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(df7.thread)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15008"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(city_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_eval_df = pd.concat([\n",
    "    get_coherence_diversity_scores(gensim_processed_posts, gensim_post_terms,fast,metric='u_mass',topic_type='lda posts'),\n",
    "    get_coherence_diversity_scores(gensim_titles_processed_posts, gensim_titles_terms,fast,metric='u_mass',topic_type='guided lda (post titles)'),\n",
    "    get_coherence_diversity_scores(gensim_degree_processed_posts, gensim_degree_terms, fast, metric='u_mass',topic_type='guided lda (high degree)'),\n",
    "    get_coherence_diversity_scores(processed_posts_init_posts, gensim_init_posts_terms,fast, metric=\"u_mass\",topic_type='guided lda (initial posts)'),\n",
    "    get_coherence_diversity_scores(gensim_processed_threads, gensim_thread_terms,fast,metric='u_mass',topic_type='lda threads'),\n",
    "    get_coherence_diversity_scores(gensim_thread_degree_processed_posts, gensim__thread_degree_terms,fast,metric='u_mass',topic_type='lda threads (high degree)'),\n",
    "    get_coherence_diversity_scores(gensim_thread_titles_processed_posts, gensim_thread_titles_terms,fast,metric='u_mass',topic_type='lda threads (post titles)'),\n",
    "    get_coherence_diversity_scores(gensim_thread_processed_posts_init_posts, gensim_thread_init_posts_terms,fast,metric='u_mass',topic_type='lda threads (initial post)'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_mass_coherence</th>\n",
       "      <th>puw:</th>\n",
       "      <th>jd:</th>\n",
       "      <th>we-cd:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lda posts</th>\n",
       "      <td>-2.132370</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.948954</td>\n",
       "      <td>0.395534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided lda (post titles)</th>\n",
       "      <td>-2.125496</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.948511</td>\n",
       "      <td>0.394123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided lda (high degree)</th>\n",
       "      <td>-2.358853</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.962835</td>\n",
       "      <td>0.345177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided lda (initial posts)</th>\n",
       "      <td>-2.093461</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.944503</td>\n",
       "      <td>0.428048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda threads</th>\n",
       "      <td>-2.131009</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.934787</td>\n",
       "      <td>0.341057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda threads (high degree)</th>\n",
       "      <td>-2.063384</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.931102</td>\n",
       "      <td>0.341057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda threads (post titles)</th>\n",
       "      <td>-2.132315</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.949102</td>\n",
       "      <td>0.341057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda threads (initial post)</th>\n",
       "      <td>-2.036049</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.942018</td>\n",
       "      <td>0.322595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            u_mass_coherence   puw:       jd:    we-cd:\n",
       "lda posts                          -2.132370  0.975  0.948954  0.395534\n",
       "guided lda (post titles)           -2.125496  0.975  0.948511  0.394123\n",
       "guided lda (high degree)           -2.358853  0.975  0.962835  0.345177\n",
       "guided lda (initial posts)         -2.093461  0.950  0.944503  0.428048\n",
       "lda threads                        -2.131009  0.900  0.934787  0.341057\n",
       "lda threads (high degree)          -2.063384  0.900  0.931102  0.341057\n",
       "lda threads (post titles)          -2.132315  0.900  0.949102  0.341057\n",
       "lda threads (initial post)         -2.036049  0.925  0.942018  0.322595"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_eval_df[['u_mass_coherence','puw:','jd:','we-cd:']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_mass_coherence</th>\n",
       "      <th>puw:</th>\n",
       "      <th>jd:</th>\n",
       "      <th>we-pd:</th>\n",
       "      <th>we-cd:</th>\n",
       "      <th>we-irbo p=0.5:</th>\n",
       "      <th>we-irbo p=0.9:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lda posts</th>\n",
       "      <td>-2.124201</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.948954</td>\n",
       "      <td>0.671730</td>\n",
       "      <td>0.395534</td>\n",
       "      <td>0.620888</td>\n",
       "      <td>0.612986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided lda (post titles)</th>\n",
       "      <td>-2.161198</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.948511</td>\n",
       "      <td>0.670787</td>\n",
       "      <td>0.394123</td>\n",
       "      <td>0.615967</td>\n",
       "      <td>0.612148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided lda (high degree)</th>\n",
       "      <td>-2.639306</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.675528</td>\n",
       "      <td>0.310433</td>\n",
       "      <td>0.660869</td>\n",
       "      <td>0.610309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guided lda (initial posts)</th>\n",
       "      <td>-2.072529</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.948188</td>\n",
       "      <td>0.667665</td>\n",
       "      <td>0.428048</td>\n",
       "      <td>0.530960</td>\n",
       "      <td>0.575025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda threads</th>\n",
       "      <td>-2.131009</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.934787</td>\n",
       "      <td>0.671317</td>\n",
       "      <td>0.341057</td>\n",
       "      <td>0.573213</td>\n",
       "      <td>0.571095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda threads (high degree)</th>\n",
       "      <td>-2.063384</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.931102</td>\n",
       "      <td>0.674843</td>\n",
       "      <td>0.341057</td>\n",
       "      <td>0.607203</td>\n",
       "      <td>0.580633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda threads (post titles)</th>\n",
       "      <td>-2.132315</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.949102</td>\n",
       "      <td>0.668235</td>\n",
       "      <td>0.341057</td>\n",
       "      <td>0.577161</td>\n",
       "      <td>0.576079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda threads (initial post)</th>\n",
       "      <td>-2.036049</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.942018</td>\n",
       "      <td>0.670593</td>\n",
       "      <td>0.322595</td>\n",
       "      <td>0.576035</td>\n",
       "      <td>0.575838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            u_mass_coherence   puw:       jd:    we-pd:  \\\n",
       "lda posts                          -2.124201  0.975  0.948954  0.671730   \n",
       "guided lda (post titles)           -2.161198  0.975  0.948511  0.670787   \n",
       "guided lda (high degree)           -2.639306  1.000  0.980392  0.675528   \n",
       "guided lda (initial posts)         -2.072529  0.950  0.948188  0.667665   \n",
       "lda threads                        -2.131009  0.900  0.934787  0.671317   \n",
       "lda threads (high degree)          -2.063384  0.900  0.931102  0.674843   \n",
       "lda threads (post titles)          -2.132315  0.900  0.949102  0.668235   \n",
       "lda threads (initial post)         -2.036049  0.925  0.942018  0.670593   \n",
       "\n",
       "                              we-cd:  we-irbo p=0.5:  we-irbo p=0.9:  \n",
       "lda posts                   0.395534        0.620888        0.612986  \n",
       "guided lda (post titles)    0.394123        0.615967        0.612148  \n",
       "guided lda (high degree)    0.310433        0.660869        0.610309  \n",
       "guided lda (initial posts)  0.428048        0.530960        0.575025  \n",
       "lda threads                 0.341057        0.573213        0.571095  \n",
       "lda threads (high degree)   0.341057        0.607203        0.580633  \n",
       "lda threads (post titles)   0.341057        0.577161        0.576079  \n",
       "lda threads (initial post)  0.322595        0.576035        0.575838  "
      ]
     },
     "execution_count": 937,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.6938108251342445"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_eval_df['u_mass_coherence'][:4].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.131735315521489"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_eval_df['u_mass_coherence'][4:].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gensim_degree_terms[:25]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gensim_post_terms[:25]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gensim_init_posts_terms).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "citydata_df.to_parquet('citydata.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Rittenhouse:Not one, but TWO 'hood' mobs attacking innocents in one of the best neighborhoods in the city. Real nice.We all know the demographics of said mobs, I just wish the media and law enforcement would say it.rotodome made a statement that you had a better chance to die in an auto accident then to be murdered in Philly. The stats in 2010 are as follows..People killed in auto accidents in the city- 84people murdered in the city- 306 (2011 was 324)so to sum it up. You are approximately 4 times as likely to be murdered in Philly then to die in an auto accident.. Interesting..\""
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_nodes + metro_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_priors = [text_tokenize(node) for node in [covid_nodes, crime_nodes + metro_nodes, plan_nodes,retail_nodes]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_posts = [init_posts[0], init_posts[1] + init_posts[2], init_posts[3], init_posts[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_posts_embeddings = smodel.encode(first_posts)\n",
    "first_posts_tokens = [text_tokenize(post) for post in first_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['everything',\n",
       " 'shutting',\n",
       " 'slowly',\n",
       " 'surely',\n",
       " 'job',\n",
       " 'shut',\n",
       " 'rest',\n",
       " 'month',\n",
       " 'home',\n",
       " 'lol',\n",
       " 'still',\n",
       " 'gotten',\n",
       " 'grocery',\n",
       " 'store',\n",
       " 'knee',\n",
       " 'replacement',\n",
       " 'considered',\n",
       " 'essential',\n",
       " 'operation',\n",
       " 'still',\n",
       " 'going',\n",
       " 'stay',\n",
       " 'well',\n",
       " 'guy']"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenize(first_posts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = [text_process(post) for post in init_posts]\n",
    "initial_posts = flatten_list([nltk.wordpunct_tokenize(i) for i in ip])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15008, 15008)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(citydata_df), len(mvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(px.scatter(x=mvecs[:,0],y=mvecs[:,1],color=citydata_df.forum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.2: 36473, 0.4: 200})"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list(etas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.ldamodel.LdaModel(\n",
    "            corpus=bow, id2word=dictionary, num_topics=5,\n",
    "            random_state=42, chunksize=100, eta=np.array(etas),\n",
    "            eval_every=-1, update_every=1,\n",
    "            passes=150, alpha='auto', per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.037*\"retail\" + 0.018*\"building\" + 0.014*\"street\" + 0.013*\"old\" + 0.013*\"mall\" + 0.012*\"love\" + 0.011*\"space\" + 0.011*\"across\" + 0.011*\"location\" + 0.010*\"hope\"'),\n",
       " (1,\n",
       "  '0.020*\"top\" + 0.020*\"luxury\" + 0.019*\"built\" + 0.014*\"company\" + 0.013*\"flagship\" + 0.011*\"story\" + 0.011*\"floor\" + 0.010*\"housing\" + 0.010*\"offer\" + 0.009*\"largest\"'),\n",
       " (2,\n",
       "  '0.018*\"traffic\" + 0.016*\"news\" + 0.013*\"demand\" + 0.012*\"help\" + 0.011*\"tax\" + 0.010*\"amount\" + 0.010*\"income\" + 0.009*\"larger\" + 0.009*\"region\" + 0.008*\"poverty\"'),\n",
       " (3,\n",
       "  '0.030*\"have\" + 0.022*\"will\" + 0.019*\"like\" + 0.017*\"more\" + 0.017*\"there\" + 0.015*\"one\" + 0.014*\"think\" + 0.012*\"city\" + 0.011*\"people\" + 0.010*\"can\"'),\n",
       " (4,\n",
       "  '0.124*\"store\" + 0.024*\"food\" + 0.022*\"department\" + 0.022*\"opened\" + 0.015*\"chain\" + 0.015*\"outlet\" + 0.014*\"line\" + 0.013*\"fashion\" + 0.009*\"service\" + 0.009*\"district\"')]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "gterms = [[token for token in m[1].split('\"') if not re.findall(r'\\d',token)] for m in  model.show_topics(num_words=25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = [float(s) for s in \"\"\"-2.590164 \n",
    "\n",
    "-2.631312 \n",
    "\n",
    "-2.104102 \n",
    "\n",
    "-2.365565 \n",
    "\n",
    "-2.107477 \n",
    "\n",
    "-2.150025 \n",
    "\n",
    "-2.324303 \n",
    "\n",
    "-2.226696 \"\"\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs =[float(s) for s in \"\"\"-4.102533 \n",
    "\n",
    "-2.761796 \n",
    "\n",
    "-2.561912 \n",
    "\n",
    "-2.530231 \n",
    "\n",
    "-5.469968 \n",
    "\n",
    "-4.842382 \n",
    "\n",
    "-4.572197 \n",
    "\n",
    "-4.266072 \"\"\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-4.102533,\n",
       " -2.761796,\n",
       " -2.561912,\n",
       " -2.530231,\n",
       " -5.469968,\n",
       " -4.842382,\n",
       " -4.572197,\n",
       " -4.266072]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.8883863749999996, 1.0571740887541816)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cs),np.std(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(px.bar(x=range(len(cs)),y=cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coherence scores.html'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.tools import FigureFactory as FF\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "\n",
    "trace1 = go.Box(\n",
    "    y=cs,\n",
    "    name = 'Box Plot',\n",
    "    boxpoints='all',\n",
    "    jitter=0.3,\n",
    "    marker = dict(\n",
    "        color = 'rgb(214,12,140)',\n",
    "    ),\n",
    ")\n",
    "trace2 = go.Box(\n",
    "    y=gs,\n",
    "    name = 'Box Plot',\n",
    "    boxpoints='all',\n",
    "    jitter=0.3,\n",
    "    marker = dict(\n",
    "        color = 'rgb(14,128,140)',\n",
    "    ),\n",
    ")\n",
    "layout = go.Layout(\n",
    "    width=500,\n",
    "    yaxis=dict(\n",
    "        title='Topical Coherence Scores',\n",
    "        zeroline=False\n",
    "    ),\n",
    ")\n",
    "\n",
    "data = [trace1,trace2]\n",
    "fig= go.Figure(layout=layout)\n",
    "fig.add_trace(trace1)\n",
    "fig.add_trace(trace2)\n",
    "plot(fig, filename='coherence scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_scores = pd.DataFrame({'SE-Topical Coherence':cs, 'LDA Topical Coherence':gs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SE-Topical Coherence</td>\n",
       "      <td>-4.102533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SE-Topical Coherence</td>\n",
       "      <td>-2.761796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SE-Topical Coherence</td>\n",
       "      <td>-2.561912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SE-Topical Coherence</td>\n",
       "      <td>-2.530231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SE-Topical Coherence</td>\n",
       "      <td>-5.469968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SE-Topical Coherence</td>\n",
       "      <td>-4.842382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SE-Topical Coherence</td>\n",
       "      <td>-4.572197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SE-Topical Coherence</td>\n",
       "      <td>-4.266072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LDA Topical Coherence</td>\n",
       "      <td>-2.590164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LDA Topical Coherence</td>\n",
       "      <td>-2.631312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LDA Topical Coherence</td>\n",
       "      <td>-2.104102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LDA Topical Coherence</td>\n",
       "      <td>-2.365565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LDA Topical Coherence</td>\n",
       "      <td>-2.107477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LDA Topical Coherence</td>\n",
       "      <td>-2.150025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LDA Topical Coherence</td>\n",
       "      <td>-2.324303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LDA Topical Coherence</td>\n",
       "      <td>-2.226696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 variable     value\n",
       "0    SE-Topical Coherence -4.102533\n",
       "1    SE-Topical Coherence -2.761796\n",
       "2    SE-Topical Coherence -2.561912\n",
       "3    SE-Topical Coherence -2.530231\n",
       "4    SE-Topical Coherence -5.469968\n",
       "5    SE-Topical Coherence -4.842382\n",
       "6    SE-Topical Coherence -4.572197\n",
       "7    SE-Topical Coherence -4.266072\n",
       "8   LDA Topical Coherence -2.590164\n",
       "9   LDA Topical Coherence -2.631312\n",
       "10  LDA Topical Coherence -2.104102\n",
       "11  LDA Topical Coherence -2.365565\n",
       "12  LDA Topical Coherence -2.107477\n",
       "13  LDA Topical Coherence -2.150025\n",
       "14  LDA Topical Coherence -2.324303\n",
       "15  LDA Topical Coherence -2.226696"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.melt(c_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(px.box(pd.melt(c_scores), x='variable',y='value',points='all',color='variable',title='Topical Coherence Scores'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.DataFrame()\n",
    "\n",
    "dff['se_puw'] = [float(s) for s in \"\"\"0.74 0.54 0.52 0.54 0.74 0.69 0.67 0.67 \"\"\".split()]\n",
    "\n",
    "dff['se_jd'] = [float(s) for s in \"\"\"0.857064 0.638784 0.620674 0.639471 0.867287 0.797508 0.785500 0.785500 \"\"\".split()]\n",
    "\n",
    "dff['se_wecd'] = [float(s) for s in \"\"\"0.060969 0.065617 0.071358 0.069196 0.129490 0.065617 0.218687 0.216487 \"\"\".split()]\n",
    "\n",
    "dff['lda_puw'] = [float(s) for s in \"\"\"1.000 1.000 0.950 0.975 0.950 0.950 0.950 0.900 \"\"\".split()]\n",
    "\n",
    "dff['lda_jd'] = [float(s) for s in \"\"\"0.959436 0.959575 0.959861 0.966373 0.959436 0.959436 0.959436 0.948797 \"\"\".split()]\n",
    "\n",
    "dff['lda_wecd'] = [float(s) for s in \"\"\"0.300081 0.277969 0.294762 0.262589 0.302670 0.313002 0.302670 0.323610 \"\"\".split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig = plot(px.box(pd.melt(dff), x='variable',y='value',points='all',color='variable',title='Topical Diversity Scores (PUW, JD, WECD)'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.93"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5.46 - 2.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_body</th>\n",
       "      <th>post</th>\n",
       "      <th>datetime</th>\n",
       "      <th>quote_id</th>\n",
       "      <th>quote_body</th>\n",
       "      <th>quote</th>\n",
       "      <th>forum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>758124</td>\n",
       "      <td>td class=\"alt1 altop\" id=\"td_post_57581240\"&gt;\\n...</td>\n",
       "      <td>So just about everything is shutting down slo...</td>\n",
       "      <td>2020-03-16 12:36:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>758139</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_57581398\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>I am so lucky to be able to work remotely thro...</td>\n",
       "      <td>2020-03-16 12:55:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>758214</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_57582145\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>I agree with really feeling for those in the s...</td>\n",
       "      <td>2020-03-16 14:26:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>758228</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_57582283\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>I'm working remotely as well. Went for booze y...</td>\n",
       "      <td>2020-03-16 14:45:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>758250</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_57582507\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>Working remotely for the foreseeable future. W...</td>\n",
       "      <td>2020-03-16 15:11:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4110</th>\n",
       "      <td>958699</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_39586996\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>So are the rumors true that AT&amp;T is looking to...</td>\n",
       "      <td>2015-05-12 13:56:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4111</th>\n",
       "      <td>958730</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_39587305\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>So are the rumors true that AT&amp;T is looking t...</td>\n",
       "      <td>2015-05-12 14:15:00</td>\n",
       "      <td>39586996</td>\n",
       "      <td>&lt;td class=\"alt2\" style=\"border:1px inset\"&gt;\\n&lt;d...</td>\n",
       "      <td>So are the rumors true that AT&amp;T is looking to...</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4112</th>\n",
       "      <td>958755</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_39587559\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>That's the rumor still. Although now I read t...</td>\n",
       "      <td>2015-05-12 14:31:00</td>\n",
       "      <td>39587305</td>\n",
       "      <td>&lt;td class=\"alt2\" style=\"border:1px inset\"&gt;\\n&lt;d...</td>\n",
       "      <td>That's the rumor still. Although now I read th...</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4113</th>\n",
       "      <td>959147</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_39591477\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>Too much vacancy and low end \"high end\" retail...</td>\n",
       "      <td>2015-05-12 19:47:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>959969</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_39599693\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>Too much vacancy and low end \"high end\" retai...</td>\n",
       "      <td>2015-05-13 12:23:00</td>\n",
       "      <td>39591477</td>\n",
       "      <td>&lt;td class=\"alt2\" style=\"border:1px inset\"&gt;\\n&lt;d...</td>\n",
       "      <td>Too much vacancy and low end \"high end\" retail...</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15008 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     post_id                                          post_body  \\\n",
       "0     758124  td class=\"alt1 altop\" id=\"td_post_57581240\">\\n...   \n",
       "1     758139  td class=\"alt1\" id=\"td_post_57581398\">\\n<!-- m...   \n",
       "2     758214  td class=\"alt1\" id=\"td_post_57582145\">\\n<!-- m...   \n",
       "3     758228  td class=\"alt1\" id=\"td_post_57582283\">\\n<!-- m...   \n",
       "4     758250  td class=\"alt1\" id=\"td_post_57582507\">\\n<!-- m...   \n",
       "...      ...                                                ...   \n",
       "4110  958699  td class=\"alt1\" id=\"td_post_39586996\">\\n<!-- m...   \n",
       "4111  958730  td class=\"alt1\" id=\"td_post_39587305\">\\n<!-- m...   \n",
       "4112  958755  td class=\"alt1\" id=\"td_post_39587559\">\\n<!-- m...   \n",
       "4113  959147  td class=\"alt1\" id=\"td_post_39591477\">\\n<!-- m...   \n",
       "4114  959969  td class=\"alt1\" id=\"td_post_39599693\">\\n<!-- m...   \n",
       "\n",
       "                                                   post             datetime  \\\n",
       "0      So just about everything is shutting down slo...  2020-03-16 12:36:00   \n",
       "1     I am so lucky to be able to work remotely thro...  2020-03-16 12:55:00   \n",
       "2     I agree with really feeling for those in the s...  2020-03-16 14:26:00   \n",
       "3     I'm working remotely as well. Went for booze y...  2020-03-16 14:45:00   \n",
       "4     Working remotely for the foreseeable future. W...  2020-03-16 15:11:00   \n",
       "...                                                 ...                  ...   \n",
       "4110  So are the rumors true that AT&T is looking to...  2015-05-12 13:56:00   \n",
       "4111   So are the rumors true that AT&T is looking t...  2015-05-12 14:15:00   \n",
       "4112   That's the rumor still. Although now I read t...  2015-05-12 14:31:00   \n",
       "4113  Too much vacancy and low end \"high end\" retail...  2015-05-12 19:47:00   \n",
       "4114   Too much vacancy and low end \"high end\" retai...  2015-05-13 12:23:00   \n",
       "\n",
       "      quote_id                                         quote_body  \\\n",
       "0                                                                   \n",
       "1                                                                   \n",
       "2                                                                   \n",
       "3                                                                   \n",
       "4                                                                   \n",
       "...        ...                                                ...   \n",
       "4110                                                                \n",
       "4111  39586996  <td class=\"alt2\" style=\"border:1px inset\">\\n<d...   \n",
       "4112  39587305  <td class=\"alt2\" style=\"border:1px inset\">\\n<d...   \n",
       "4113                                                                \n",
       "4114  39591477  <td class=\"alt2\" style=\"border:1px inset\">\\n<d...   \n",
       "\n",
       "                                                  quote        forum  \n",
       "0                                                        coronavirus  \n",
       "1                                                        coronavirus  \n",
       "2                                                        coronavirus  \n",
       "3                                                        coronavirus  \n",
       "4                                                        coronavirus  \n",
       "...                                                 ...          ...  \n",
       "4110                                                          retail  \n",
       "4111  So are the rumors true that AT&T is looking to...       retail  \n",
       "4112  That's the rumor still. Although now I read th...       retail  \n",
       "4113                                                          retail  \n",
       "4114  Too much vacancy and low end \"high end\" retail...       retail  \n",
       "\n",
       "[15008 rows x 8 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citydata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\rmomi\\\\OneDrive\\\\Desktop\\\\city-data2'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is A2DB-0107\n",
      "\n",
      " Directory of C:\\Users\\rmomi\\OneDrive\\Desktop\\city-data2\n",
      "\n",
      "11/08/2023  06:14 PM    <DIR>          .\n",
      "11/01/2023  09:42 PM    <DIR>          ..\n",
      "10/28/2023  10:43 PM    <DIR>          .ipynb_checkpoints\n",
      "10/20/2023  11:57 AM    <DIR>          __pycache__\n",
      "04/13/2022  05:41 PM        64,928,417 20newsgroups_tagged_df.pkl\n",
      "10/06/2023  08:23 PM    <DIR>          amzhq\n",
      "10/06/2023  08:23 PM    <DIR>          beingwhite\n",
      "02/09/2020  07:46 AM         4,252,000 city_data_crime.csv\n",
      "02/11/2020  07:34 AM               442 city_data_dscope.py\n",
      "01/18/2020  10:26 AM        16,212,603 city_data_philly2035.csv\n",
      "01/18/2020  01:51 PM        88,648,466 city_data_philly2035_dscope.pkl\n",
      "01/19/2020  11:16 AM           678,499 city_data_philly2035_dscope_lats.pkl\n",
      "01/19/2020  11:56 AM        72,623,418 city_data_philly2035_tagged_scored.csv\n",
      "04/15/2020  08:58 PM            10,208 city_data_scrape.py\n",
      "01/17/2020  11:40 AM         8,432,028 city_data0.csv\n",
      "11/05/2023  12:25 PM        64,481,032 citydata.csv\n",
      "11/05/2023  12:24 PM        17,760,296 citydata.parquet\n",
      "11/05/2023  12:26 PM         7,065,380 citydata.xlsx\n",
      "10/22/2023  12:05 AM         8,165,380 citydata-2035plan.parquet\n",
      "10/11/2023  04:38 PM         8,208,311 city-data-2035plan.parquet\n",
      "10/22/2023  12:05 AM           612,265 citydata-covid.parquet\n",
      "10/22/2023  12:05 AM         2,842,851 citydata-crime.parquet\n",
      "10/11/2023  04:38 PM         2,022,021 city-data-crime.parquet\n",
      "10/07/2023  01:42 PM         4,865,874 city-data-crime.pkl\n",
      "01/19/2020  11:55 AM         5,135,227 CITY-DATA-FACTORS.ipynb\n",
      "02/09/2020  09:34 AM         2,055,073 city-data-metro.csv\n",
      "10/22/2023  12:05 AM         1,139,878 citydata-metro.parquet\n",
      "10/11/2023  04:38 PM           892,092 city-data-metro.parquet\n",
      "10/07/2023  01:42 PM         2,331,667 city-data-metro.pkl\n",
      "10/28/2023  11:30 PM           248,157 city-data-philadelphia.ipynb\n",
      "10/20/2023  11:05 AM         4,937,123 city-data-philly2035.ipynb\n",
      "10/07/2023  01:42 PM        18,223,795 city-data-philly2035.pkl\n",
      "02/10/2020  05:43 AM         4,252,000 city-data-philly-crime.csv\n",
      "02/10/2020  05:48 AM         2,055,073 city-data-philly-metro.csv\n",
      "10/30/2023  03:28 PM           289,382 city-data-philly-processing.ipynb\n",
      "11/08/2023  06:14 PM           219,418 city-data-philly-processing-final.ipynb\n",
      "11/04/2023  09:18 PM           266,875 city-data-philly-topical-coherence-topoi.ipynb\n",
      "02/11/2020  07:30 AM         8,415,151 city-data-retail.csv\n",
      "10/22/2023  12:05 AM         5,115,744 citydata-retail.parquet\n",
      "10/11/2023  04:38 PM         3,981,893 city-data-retail.parquet\n",
      "10/07/2023  01:42 PM         9,552,276 city-data-retail.pkl\n",
      "11/08/2023  02:36 PM         3,690,774 coherence scores.html\n",
      "10/06/2023  08:23 PM    <DIR>          covid-newyork\n",
      "10/06/2023  08:23 PM    <DIR>          covid-philly\n",
      "10/16/2023  06:13 PM    <DIR>          crawl-300d-2M-subword\n",
      "10/22/2023  09:03 PM     5,828,358,084 crawl-300d-2M-subword.zip\n",
      "10/06/2023  08:23 PM    <DIR>          crime\n",
      "01/18/2020  10:24 AM        13,361,381 data0.csv\n",
      "10/06/2023  08:23 PM    <DIR>          metro\n",
      "10/06/2023  08:23 PM    <DIR>          njconstruction\n",
      "10/06/2023  08:23 PM    <DIR>          njwestmont\n",
      "10/25/2023  07:36 PM    <DIR>          philly2035_0_400\n",
      "10/06/2023  08:23 PM    <DIR>          retail\n",
      "11/08/2023  03:26 PM         3,691,932 temp-plot.html\n",
      "10/06/2023  08:23 PM    <DIR>          trumpeconomy\n",
      "10/28/2023  10:39 PM           223,833 Untitled.ipynb\n",
      "10/13/2023  03:06 PM    <DIR>          Wikipedia_bd\n",
      "              40 File(s)  6,290,246,319 bytes\n",
      "              17 Dir(s)  694,128,254,976 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_posts = [\"\"\"For every one of those articles (written by beer lovers), there is a scientific article that explains the facts of alcohol (carcinogen) and its negative health impacts. I have drank plenty in my life, and still enjoy a beer or wine, but I'm not fooling myself into thinking its a life-sustaining entity. The same people who complain about not making enough money or not being able to pay the rent will go out and spend their limited cash on booze.  How much of the fed's cash payments will go to alcohol?\"\"\"]\n",
    "\n",
    "[citydata_df.iloc[i] for i in range(len(citydata_df)) if re.findall('For every one of those articles (written by beer lovers), there is a scientific article',citydata_df.iloc[i].post)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_body</th>\n",
       "      <th>post</th>\n",
       "      <th>datetime</th>\n",
       "      <th>quote_id</th>\n",
       "      <th>quote_body</th>\n",
       "      <th>quote</th>\n",
       "      <th>forum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>758124</td>\n",
       "      <td>td class=\"alt1 altop\" id=\"td_post_57581240\"&gt;\\n...</td>\n",
       "      <td>So just about everything is shutting down slo...</td>\n",
       "      <td>2020-03-16 12:36:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>758139</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_57581398\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>I am so lucky to be able to work remotely thro...</td>\n",
       "      <td>2020-03-16 12:55:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>758214</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_57582145\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>I agree with really feeling for those in the s...</td>\n",
       "      <td>2020-03-16 14:26:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>758228</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_57582283\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>I'm working remotely as well. Went for booze y...</td>\n",
       "      <td>2020-03-16 14:45:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>758250</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_57582507\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>Working remotely for the foreseeable future. W...</td>\n",
       "      <td>2020-03-16 15:11:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4110</th>\n",
       "      <td>958699</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_39586996\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>So are the rumors true that AT&amp;T is looking to...</td>\n",
       "      <td>2015-05-12 13:56:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4111</th>\n",
       "      <td>958730</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_39587305\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>So are the rumors true that AT&amp;T is looking t...</td>\n",
       "      <td>2015-05-12 14:15:00</td>\n",
       "      <td>39586996</td>\n",
       "      <td>&lt;td class=\"alt2\" style=\"border:1px inset\"&gt;\\n&lt;d...</td>\n",
       "      <td>So are the rumors true that AT&amp;T is looking to...</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4112</th>\n",
       "      <td>958755</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_39587559\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>That's the rumor still. Although now I read t...</td>\n",
       "      <td>2015-05-12 14:31:00</td>\n",
       "      <td>39587305</td>\n",
       "      <td>&lt;td class=\"alt2\" style=\"border:1px inset\"&gt;\\n&lt;d...</td>\n",
       "      <td>That's the rumor still. Although now I read th...</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4113</th>\n",
       "      <td>959147</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_39591477\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>Too much vacancy and low end \"high end\" retail...</td>\n",
       "      <td>2015-05-12 19:47:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>959969</td>\n",
       "      <td>td class=\"alt1\" id=\"td_post_39599693\"&gt;\\n&lt;!-- m...</td>\n",
       "      <td>Too much vacancy and low end \"high end\" retai...</td>\n",
       "      <td>2015-05-13 12:23:00</td>\n",
       "      <td>39591477</td>\n",
       "      <td>&lt;td class=\"alt2\" style=\"border:1px inset\"&gt;\\n&lt;d...</td>\n",
       "      <td>Too much vacancy and low end \"high end\" retail...</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15008 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     post_id                                          post_body  \\\n",
       "0     758124  td class=\"alt1 altop\" id=\"td_post_57581240\">\\n...   \n",
       "1     758139  td class=\"alt1\" id=\"td_post_57581398\">\\n<!-- m...   \n",
       "2     758214  td class=\"alt1\" id=\"td_post_57582145\">\\n<!-- m...   \n",
       "3     758228  td class=\"alt1\" id=\"td_post_57582283\">\\n<!-- m...   \n",
       "4     758250  td class=\"alt1\" id=\"td_post_57582507\">\\n<!-- m...   \n",
       "...      ...                                                ...   \n",
       "4110  958699  td class=\"alt1\" id=\"td_post_39586996\">\\n<!-- m...   \n",
       "4111  958730  td class=\"alt1\" id=\"td_post_39587305\">\\n<!-- m...   \n",
       "4112  958755  td class=\"alt1\" id=\"td_post_39587559\">\\n<!-- m...   \n",
       "4113  959147  td class=\"alt1\" id=\"td_post_39591477\">\\n<!-- m...   \n",
       "4114  959969  td class=\"alt1\" id=\"td_post_39599693\">\\n<!-- m...   \n",
       "\n",
       "                                                   post             datetime  \\\n",
       "0      So just about everything is shutting down slo...  2020-03-16 12:36:00   \n",
       "1     I am so lucky to be able to work remotely thro...  2020-03-16 12:55:00   \n",
       "2     I agree with really feeling for those in the s...  2020-03-16 14:26:00   \n",
       "3     I'm working remotely as well. Went for booze y...  2020-03-16 14:45:00   \n",
       "4     Working remotely for the foreseeable future. W...  2020-03-16 15:11:00   \n",
       "...                                                 ...                  ...   \n",
       "4110  So are the rumors true that AT&T is looking to...  2015-05-12 13:56:00   \n",
       "4111   So are the rumors true that AT&T is looking t...  2015-05-12 14:15:00   \n",
       "4112   That's the rumor still. Although now I read t...  2015-05-12 14:31:00   \n",
       "4113  Too much vacancy and low end \"high end\" retail...  2015-05-12 19:47:00   \n",
       "4114   Too much vacancy and low end \"high end\" retai...  2015-05-13 12:23:00   \n",
       "\n",
       "      quote_id                                         quote_body  \\\n",
       "0                                                                   \n",
       "1                                                                   \n",
       "2                                                                   \n",
       "3                                                                   \n",
       "4                                                                   \n",
       "...        ...                                                ...   \n",
       "4110                                                                \n",
       "4111  39586996  <td class=\"alt2\" style=\"border:1px inset\">\\n<d...   \n",
       "4112  39587305  <td class=\"alt2\" style=\"border:1px inset\">\\n<d...   \n",
       "4113                                                                \n",
       "4114  39591477  <td class=\"alt2\" style=\"border:1px inset\">\\n<d...   \n",
       "\n",
       "                                                  quote        forum  \n",
       "0                                                        coronavirus  \n",
       "1                                                        coronavirus  \n",
       "2                                                        coronavirus  \n",
       "3                                                        coronavirus  \n",
       "4                                                        coronavirus  \n",
       "...                                                 ...          ...  \n",
       "4110                                                          retail  \n",
       "4111  So are the rumors true that AT&T is looking to...       retail  \n",
       "4112  That's the rumor still. Although now I read th...       retail  \n",
       "4113                                                          retail  \n",
       "4114  Too much vacancy and low end \"high end\" retail...       retail  \n",
       "\n",
       "[15008 rows x 8 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citydata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'758124'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid.post_id.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'908813'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime.post_id.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'251839'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metro.post_id.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'958364'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan.post_id.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'710692'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retail.post_id.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "O0WSbufRTKcb"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
